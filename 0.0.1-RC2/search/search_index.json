{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Shim - 0.0.1-RC2","text":"Coverage<p> Statement 0.00 Branch 0.00 </p>"},{"location":"#provides-shims-over-private-apache-spark-apis","title":"Provides shims over private Apache Spark APIs","text":"<p>Using the private Spark APIs can allow for improved functionality (e.g. Frameless encoder derivation) and performance (e.g. Quality HOF compilation or Plan rules) but it comes at a cost: lots of workarounds to support different runtimes.</p> <p>Shim aims to bring those workarounds in a separate runtime library shim and let the library authors focus on useful functionality.</p>"},{"location":"reference_material/","title":"Landing","text":"<p>The Shim reference material can be found on the left &lt;\u2013 </p>"},{"location":"background/about/","title":"History","text":""},{"location":"background/about/#why-shim","title":"Why Shim?","text":"<p>In short - both the OSS and Vendor Spark teams want to innovate without needing to think about internal api changes that shouldn't affect 95% of the user base.</p> <p>Frameless and Quality users likely feel the same, but changes made in Spark's internal apis can break both libraries through linkage errors, missing functions etc. and more insidious issues such as changes in decimal type handling.</p> <p>The straw that broke the camels proverbial was a change to StaticInvoke made in Spark 4, which was reasonably added to the Databricks 14.2, the only 3.5 build.  This change added a new parameter which works fine if you recompile but breaks at runtime if you built against 3.5.  So although both Frameless and Quality work flawlessly on 14.0 and 14.1, they don't work on 14.2.</p> <p>Shim aims to alleviate this pain by swapping out the different runtime implementations as needed.  </p>"},{"location":"background/changelog/","title":"Changelog","text":""},{"location":"background/changelog/#001-rc2-8th-march-2024","title":"0.0.1-RC2 8th March, 2024","text":"<p>#1 - Quality support</p> <p>#2 - Frameless support</p> <p>#3 - 14.3 LTS support</p>"},{"location":"getting_started/","title":"Building and Setting Up","text":"","tags":["basic","getting started","beginner"]},{"location":"getting_started/#building-the-library","title":"Building The Library","text":"<ul> <li>fork, </li> <li>use the Scala dev environment of your choice,</li> <li>or build directly using Maven</li> </ul>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#building-via-commandline","title":"Building via commandline","text":"<p>For OSS versions (non Databricks runtime - dbr):</p> <pre><code>mvn --batch-mode --errors --fail-at-end --show-version -DinstallAtEnd=true -DdeployAtEnd=true -DskipTests install -P Spark321\n</code></pre> <p>but dbr versions will not be able to run tests from the command line (typically not an issue in intellij):</p> <pre><code>mvn --batch-mode --errors --fail-at-end --show-version -DinstallAtEnd=true -DdeployAtEnd=true -DskipTests clean install -P 10.4.dbr\n</code></pre>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#build-tool-dependencies","title":"Build tool dependencies","text":"<p>Shim is cross compiled for different versions of Spark, Scala and runtimes such as Databricks.  The format for artifact's is:</p> <pre><code>shim_[compilation|runtime]_RUNTIME_SPARKCOMPATVERSION_SCALACOMPATVERSION-VERSION.jar\n</code></pre> <p>e.g.</p> <pre><code>shim_runtime_3.4.1.oss_3.4_2.12-0.1.3.jar\n</code></pre> <p>The build poms generate those variables via maven profiles, but you are advised to use properties to configure e.g. for Maven:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;com.sparkutils&lt;/groupId&gt;\n&lt;artifactId&gt;shim_runtime_${qualityRuntime}${sparkShortVersion}_${scalaCompatVersion}&lt;/artifactId&gt;\n&lt;version&gt;${shimRuntimeVersion}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The \"compilation\" artefacts are only needed if you rely on the internal apis used in them (e.g. Frameless doesn't, Quality does), you should attempt the use of runtime only first.</p> <p>The full list of supported runtimes is below:</p> Spark Version sparkShortVersion qualityRuntime scalaCompatVersion 2.4.6 2.4 2.11 3.0.3 3.0 2.12 3.1.3 3.1 2.12 3.1.3 3.1 9.1.dbr_ 2.12 3.2.0 3.2 2.12, 2.13 3.2.1 3.2 3.2.1.oss_ 2.12, 2.13 3.2.1 3.2 10.4.dbr_ 2.12, 2.13 3.3.2 3.3 3.3.2.oss_ 2.12, 2.13 3.3.2 3.3 11.3.dbr_ 2.12, 2.13 3.3.2 3.3 12.2.dbr_ 2.12, 2.13 3.3.2 3.3 13.1.dbr_ 2.12, 2.13 3.4.1 3.4 3.4.1.oss_ 2.12, 2.13 3.4.1 3.4 13.1.dbr_ 2.12, 2.13 3.4.1 3.4 13.3.dbr_ 2.12, 2.13 3.5.0 3.5 3.5.0.oss_ 2.12, 2.13 3.5.0 3.5 14.0.dbr_ 2.12, 2.13 3.5.0 3.5 14.3.dbr_ 2.12, 2.13 <p>2.4 support is deprecated and will be removed in a future version.  3.1.2 support is replaced by 3.1.3 due to interpreted encoder issues. </p> <p>Databricks 13.x support</p> <p>13.0 also works on the 12.2.dbr_ build as of 10th May 2023, despite the Spark version difference.</p> <p>13.1 requires its own version as it backports 3.5 functionality.  The 13.1.dbr quality runtime build also works on 13.2 DBR.</p> <p>13.3 has backports of 4.0 functionality which requires it's own runtime.</p> <p>Databricks 14.x support</p> <p>Due to back-porting of SPARK-44913 frameless 0.16.0 (the 3.5.0 release) is not binary compatible with 14.2 and above which has back-ported this 4.0 interface change. Similarly, 4.0 / 14.2 introduces a change in resolution so a new runtime version is required upon a potential fix for 44913 in frameless.</p> <p>14.2 is not directly supported but has been tested and works with the 14.3 LTS release.</p> <p>Use the 14.3 version on 14.3, 14.0.dbr will not work</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#developing-for-a-databricks-runtime","title":"Developing for a Databricks Runtime","text":"<p>As there are many compatibility issues that Shim works around between the various Spark runtimes and their Databricks equivalents you will need to use two different runtimes when you do local testing (and of course you should do that):</p> <pre><code>&lt;properties&gt;\n&lt;shimRuntimeVersion&gt;0.1.3&lt;/shimRuntimeVersion&gt;\n&lt;shimRuntimeTest&gt;3.4.1.oss_&lt;/shimRuntimeTest&gt;\n&lt;shimRuntimeDatabricks&gt;13.1.dbr_&lt;/shimRuntimeDatabricks&gt;\n&lt;sparkShortVersion&gt;3.4&lt;/sparkShortVersion&gt;\n&lt;scalaCompatVersion&gt;2.12&lt;/scalaCompatVersion&gt;    &lt;/properties&gt;\n\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.sparkutils.&lt;/groupId&gt;\n&lt;artifactId&gt;shim_runtime_${shimRuntimeTest}${sparkShortVersion}_${scalaCompatVersion}&lt;/artifactId&gt;\n&lt;version&gt;${shimRuntimeVersion}&lt;/version&gt;\n&lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.sparkutils&lt;/groupId&gt;\n&lt;artifactId&gt;shim_runtime_${shimRuntimeDatabricks}${sparkShortVersion}_${scalaCompatVersion}&lt;/artifactId&gt;\n&lt;version&gt;${shimRuntimeVersion}&lt;/version&gt;\n&lt;scope&gt;compile&lt;/scope&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> <p>That horrific looking \".\" on the test groupId is required to get Maven 3 to use different versions many thanks for finding this Zheng.</p> <p>It's safe to assume better build tools like gradle / sbt do not need such hackery. </p> <p>The known combinations requiring this approach is below:</p> Spark Version sparkShortVersion qualityTestPrefix qualityDatabricksPrefix scalaCompatVersion 3.2.1 3.2 3.2.1.oss_ 10.4.dbr_ 2.12 3.3.0 3.3 3.3.0.oss_ 11.3.dbr_ 2.12 3.3.2 3.3 3.3.2.oss_ 12.2.dbr_ 2.12 3.4.1 3.4 3.4.1.oss_ 13.1.dbr_ 2.12 3.4.1 3.4 3.4.1.oss_ 13.3.dbr_ 2.12 3.5.0 3.5 3.5.0.oss_ 14.0.dbr_ 2.12 3.5.0 3.5 3.5.0.oss_ 14.3.dbr_ 2.12 <p>Only 2.12 is supported on DBRs</p> <p>Although the build is run for 2.13 it's not expected to be used until a later runtime based on 4.0</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#developing-a-library-against-internal-apis-changed-by-databricks","title":"Developing a library against internal APIs changed by Databricks","text":"<p>In this scenario, similar to Quality, it is assumed you want to use internal apis covered in the version specific \"compilation\" source.  The approach taken is to force Databricks runtime compatible interfaces higher up in the classpath than the OSS equivalents (or indeed provide them where the OSS version doesn't have them - like backported code from as yet unreleased OSS versions).</p> <p>This approach requires your build tool environment to support runtime ordering in the build, if it does then you may simply depend on the shim_compilation artefact as provided scope.  The scala maven plugin does not maintain order from maven, which is fine for most usages, just not this one....</p> <p>In order to support maven some config is needed - for a working complete build see Quality's - namely to use the sources classifier with the dependency and build helper plugins:</p> <pre><code>&lt;project&gt;\n...\n    &lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.sparkutils&lt;/groupId&gt;\n&lt;artifactId&gt;shim_compilation_${shimCompilationRuntime}_${sparkCompatVersion}_${scalaCompatVersion}&lt;/artifactId&gt;\n&lt;version&gt;${shimCompilationVersion}&lt;/version&gt;\n&lt;scope&gt;provided&lt;/scope&gt;\n&lt;classifier&gt;sources&lt;/classifier&gt;\n&lt;exclusions&gt;\n&lt;exclusion&gt;\n&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n&lt;artifactId&gt;spark-sql_${scalaCompatVersion}&lt;/artifactId&gt;\n&lt;/exclusion&gt;\n&lt;/exclusions&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.sparkutils&lt;/groupId&gt;\n&lt;artifactId&gt;shim_runtime_${shimRuntime}_${sparkCompatVersion}_${scalaCompatVersion}&lt;/artifactId&gt;\n&lt;version&gt;${shimRuntimeVersion}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n\n&lt;plugins&gt;\n&lt;plugin&gt;\n&lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;\n&lt;artifactId&gt;build-helper-maven-plugin&lt;/artifactId&gt;\n&lt;version&gt;${buildHelperPluginVersion}&lt;/version&gt;\n&lt;executions&gt;\n&lt;execution&gt;\n&lt;id&gt;add-source&lt;/id&gt;\n&lt;phase&gt;generate-sources&lt;/phase&gt;\n&lt;goals&gt;\n&lt;goal&gt;add-source&lt;/goal&gt;\n&lt;/goals&gt;\n&lt;configuration&gt;\n&lt;sources&gt;\n&lt;source&gt;src/main/scala&lt;/source&gt;\n&lt;source&gt;src/main/${profileDir}-scala&lt;/source&gt;\n&lt;source&gt;${project.build.directory}/shim_compilation_${shimCompilationRuntime}_${sparkCompatVersion}_${scalaCompatVersion}&lt;/source&gt;\n&lt;/sources&gt;\n&lt;/configuration&gt;\n&lt;/execution&gt;\n...\n    &lt;/executions&gt;\n&lt;/plugin&gt;\n\n&lt;plugin&gt;\n&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n&lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;\n&lt;version&gt;${dependencyPluginVersion}&lt;/version&gt;\n&lt;executions&gt; &lt;!-- maven scala plugin uses a set to store classpath, it doesn't follow maven's so we need to use the source --&gt;\n&lt;execution&gt;\n&lt;id&gt;unpack&lt;/id&gt;\n&lt;phase&gt;initialize&lt;/phase&gt;\n&lt;goals&gt;\n&lt;goal&gt;unpack&lt;/goal&gt;\n&lt;/goals&gt;\n&lt;configuration&gt;\n&lt;artifactItems&gt;\n&lt;artifactItem&gt;\n&lt;groupId&gt;com.sparkutils&lt;/groupId&gt;\n&lt;artifactId&gt;shim_compilation_${shimCompilationRuntime}_${sparkCompatVersion}_${scalaCompatVersion}&lt;/artifactId&gt;\n&lt;version&gt;${shimCompilationVersion}&lt;/version&gt;\n\n&lt;classifier&gt;sources&lt;/classifier&gt;\n&lt;type&gt;jar&lt;/type&gt;\n\n&lt;overWrite&gt;true&lt;/overWrite&gt;\n&lt;outputDirectory&gt;${project.build.directory}/shim_compilation_${shimCompilationRuntime}_${sparkCompatVersion}_${scalaCompatVersion}&lt;/outputDirectory&gt;\n&lt;/artifactItem&gt;\n&lt;/artifactItems&gt;\n&lt;/configuration&gt;\n&lt;/execution&gt;\n&lt;/executions&gt;\n&lt;/plugin&gt;\n&lt;/plugins&gt;\n&lt;/project&gt;\n</code></pre> <p>This, at project initialization phase, downloads and unpacks the shim_compilation correct version to the target directory (the dependency plugin configuration) and then, at source generation phase, adds the directory as source with the build-helper plugin.  </p> <p>how will I know if I need this?</p> <p>You'll get strange errors, incompatible implementations or linkages, missing methods etc.  Hopefully they are already covered by the current code, if not raise an issue and we'll see if there is a solution to it.  </p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#using-a-library-that-depends-on-shim","title":"Using a library that depends on Shim","text":"<p>If you are using, for example, Frameless or Quality, you need to understand if it has a dependency on shim_compilation or just runtime.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#runtime-dependency","title":"Runtime dependency","text":"<p>Frameless, for example, uses runtime only, so you can possibly swap it out for other runtimes, you'll need to exclude the dependency e.g.:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.typelevel&lt;/groupId&gt;\n&lt;artifactId&gt;frameless-core_${scalaCompatVersion}&lt;/artifactId&gt;\n&lt;version&gt;${framelessVersion}&lt;/version&gt;\n&lt;exclusions&gt;\n&lt;exclusion&gt;\n&lt;groupId&gt;com.sparkutils&lt;/groupId&gt;\n&lt;artifactId&gt;*&lt;/artifactId&gt;\n&lt;/exclusion&gt;\n&lt;/exclusions&gt;\n&lt;/dependency&gt;\n</code></pre> <p>You can then depend on the appropriate runtimes.</p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/#compilation-dependency","title":"Compilation dependency","text":"<p>Quality uses a compilation dependency with sources classifier that is not passed on to users of the library.  However, given it uses compilation, you likely cannot change different shim_runtime's than provided by a particular Quality runtime.  </p>","tags":["basic","getting started","beginner"]},{"location":"getting_started/running_on_databricks/","title":"Running Shim on Databricks","text":"<p>In short, you should generally be able to use the OSS shim_runtime jars to build against and use the appropriate DBR version to run against - unless you are using internal apis that Databricks has changed then you need the shim_compilation approach (see here for details).</p> <p>The shims are currently tested under Quality tests (which tests most, but not all Frameless encoding) and, as such, there is currently no direct tests for the shims themselves (this is tracked under #4)</p>","tags":["basic","getting started","beginner"]}]}