<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          org/apache/spark/sql/shim/hash/Hash.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier, monospace;'>1 <span style=''>package org.apache.spark.sql.shim.hash
</span>2 <span style=''>
</span>3 <span style=''>import org.apache.spark.sql.ShimUtils
</span>4 <span style=''>import org.apache.spark.sql.catalyst.InternalRow
</span>5 <span style=''>import org.apache.spark.sql.catalyst.analysis.TypeCheckResult
</span>6 <span style=''>import org.apache.spark.sql.catalyst.expressions.Expression
</span>7 <span style=''>import org.apache.spark.sql.catalyst.expressions.codegen._
</span>8 <span style=''>import org.apache.spark.sql.catalyst.util.{ArrayData, GenericArrayData, MapData}
</span>9 <span style=''>import org.apache.spark.sql.types._
</span>10 <span style=''>import org.apache.spark.unsafe.Platform
</span>11 <span style=''>import org.apache.spark.unsafe.types.{CalendarInterval, UTF8String}
</span>12 <span style=''>
</span>13 <span style=''>import scala.annotation.tailrec
</span>14 <span style=''>
</span>15 <span style=''>//CTw - this is copied 1:1 from the main dist replacing E and Long with Array[Long] for variable length hashes
</span>16 <span style=''>// seed gets replaced with a type that returns Array[Long] and includes generation / reset for each new digest with
</span>17 <span style=''>// a clear MessageDigest impl
</span>18 <span style=''>
</span>19 <span style=''>/**
</span>20 <span style=''> * Basic digest implementation for Array[Long] based hashes
</span>21 <span style=''> */
</span>22 <span style=''>trait Digest {
</span>23 <span style=''>  def hashInt(i: Int): Unit
</span>24 <span style=''>
</span>25 <span style=''>  def hashLong(l: Long): Unit
</span>26 <span style=''>
</span>27 <span style=''>  def hashBytes(base: Array[Byte], offset: Int, length: Int): Unit
</span>28 <span style=''>
</span>29 <span style=''>  def digest: Array[Long]
</span>30 <span style=''>}
</span>31 <span style=''>
</span>32 <span style=''>/**
</span>33 <span style=''> * Factory to get a new or reset digest for each row
</span>34 <span style=''> */
</span>35 <span style=''>trait DigestFactory extends Serializable {
</span>36 <span style=''>  def fresh: Digest
</span>37 <span style=''>  def length: Int
</span>38 <span style=''>}
</span>39 <span style=''>
</span>40 <span style=''>/**
</span>41 <span style=''> * A function that calculates hash value for a group of expressions.  Note that the `seed` argument
</span>42 <span style=''> * is not exposed to users and should only be set inside spark SQL.
</span>43 <span style=''> *
</span>44 <span style=''> * The hash value for an expression depends on its type and seed:
</span>45 <span style=''> *  - null:                    seed
</span>46 <span style=''> *  - boolean:                 turn boolean into int, 1 for true, 0 for false,
</span>47 <span style=''> *                             and then use murmur3 to hash this int with seed.
</span>48 <span style=''> *  - byte, short, int:        use murmur3 to hash the input as int with seed.
</span>49 <span style=''> *  - long:                    use murmur3 to hash the long input with seed.
</span>50 <span style=''> *  - float:                   turn it into int: java.lang.Float.floatToIntBits(input), and hash it.
</span>51 <span style=''> *  - double:                  turn it into long: java.lang.Double.doubleToLongBits(input),
</span>52 <span style=''> *                             and hash it.
</span>53 <span style=''> *  - decimal:                 if it's a small decimal, i.e. precision &lt;= 18, turn it into long
</span>54 <span style=''> *                             and hash it. Else, turn it into bytes and hash it.
</span>55 <span style=''> *  - calendar interval:       hash `microseconds` first, and use the result as seed
</span>56 <span style=''> *                             to hash `months`.
</span>57 <span style=''> *  - interval day to second:  it store long value of `microseconds`, use murmur3 to hash the long
</span>58 <span style=''> *                             input with seed.
</span>59 <span style=''> *  - interval year to month:  it store int value of `months`, use murmur3 to hash the int
</span>60 <span style=''> *                             input with seed.
</span>61 <span style=''> *  - binary:                  use murmur3 to hash the bytes with seed.
</span>62 <span style=''> *  - string:                  get the bytes of string and hash it.
</span>63 <span style=''> *  - array:                   The `result` starts with seed, then use `result` as seed, recursively
</span>64 <span style=''> *                             calculate hash value for each element, and assign the element hash
</span>65 <span style=''> *                             value to `result`.
</span>66 <span style=''> *  - struct:                  The `result` starts with seed, then use `result` as seed, recursively
</span>67 <span style=''> *                             calculate hash value for each field, and assign the field hash value
</span>68 <span style=''> *                             to `result`.
</span>69 <span style=''> *
</span>70 <span style=''> * Finally we aggregate the hash values for each expression by the same way of struct.
</span>71 <span style=''> */
</span>72 <span style=''>abstract class HashLongsExpression extends Expression with CodegenFallback {
</span>73 <span style=''>  val factory: DigestFactory
</span>74 <span style=''>
</span>75 <span style=''>  val asStruct: Boolean
</span>76 <span style=''>
</span>77 <span style=''>  override def dataType: DataType =
</span>78 <span style=''>    if (</span><span style='background: #F0ADAD'>asStruct</span><span style=''>)
</span>79 <span style=''>      </span><span style='background: #F0ADAD'>StructType(
</span>80 <span style=''></span><span style='background: #F0ADAD'>        (0 until factory.length).map(i =&gt; StructField(name = &quot;i&quot;+i, dataType = LongType))
</span>81 <span style=''></span><span style='background: #F0ADAD'>      )</span><span style=''>
</span>82 <span style=''>    else
</span>83 <span style=''>      </span><span style='background: #F0ADAD'>ArrayType(LongType)</span><span style=''>
</span>84 <span style=''>
</span>85 <span style=''>  override def foldable: Boolean = </span><span style='background: #F0ADAD'>children.forall(_.foldable)</span><span style=''>
</span>86 <span style=''>
</span>87 <span style=''>  override def nullable: Boolean = </span><span style='background: #F0ADAD'>false</span><span style=''>
</span>88 <span style=''>
</span>89 <span style=''>  private def hasMapType(dt: DataType): Boolean = {
</span>90 <span style=''>    </span><span style='background: #F0ADAD'>dt.existsRecursively(_.isInstanceOf[MapType])</span><span style=''>
</span>91 <span style=''>  }
</span>92 <span style=''>
</span>93 <span style=''>  override def checkInputDataTypes(): TypeCheckResult = {
</span>94 <span style=''>    if (</span><span style='background: #F0ADAD'>children.length &lt; 1</span><span style=''>) {
</span>95 <span style=''>      </span><span style='background: #F0ADAD'>TypeCheckResult.TypeCheckFailure(
</span>96 <span style=''></span><span style='background: #F0ADAD'>        s&quot;input to function $prettyName requires at least one argument&quot;)</span><span style=''>
</span>97 <span style=''>    } /*
</span>98 <span style=''>original code but we'll assume it can't be disabled
</span>99 <span style=''>    else if (children.exists(child =&gt; hasMapType(child.dataType)) &amp;&amp;
</span>100 <span style=''>      !SQLConf.get.getConf(SQLConf.LEGACY_ALLOW_HASH_ON_MAPTYPE)) {
</span>101 <span style=''>
</span>102 <span style=''>      TypeCheckResult.TypeCheckFailure(
</span>103 <span style=''>        s&quot;input to function $prettyName cannot contain elements of MapType. In Spark, same maps &quot; +
</span>104 <span style=''>          &quot;may have different hashcode, thus hash expressions are prohibited on MapType elements.&quot; +
</span>105 <span style=''>          s&quot; To restore previous behavior set ${SQLConf.LEGACY_ALLOW_HASH_ON_MAPTYPE.key} &quot; +
</span>106 <span style=''>          &quot;to true.&quot;)
</span>107 <span style=''>       */
</span>108 <span style=''>    else </span><span style='background: #F0ADAD'>if (children.exists(child =&gt; hasMapType(child.dataType))) {
</span>109 <span style=''></span><span style='background: #F0ADAD'>      TypeCheckResult.TypeCheckFailure(
</span>110 <span style=''></span><span style='background: #F0ADAD'>        s&quot;input to function $prettyName cannot contain elements of MapType. In Spark, same maps &quot; +
</span>111 <span style=''></span><span style='background: #F0ADAD'>          &quot;may have different hashcode, thus hash expressions are prohibited on MapType elements.&quot;)
</span>112 <span style=''></span><span style='background: #F0ADAD'>    } else {
</span>113 <span style=''></span><span style='background: #F0ADAD'>      TypeCheckResult.TypeCheckSuccess
</span>114 <span style=''></span><span style='background: #F0ADAD'>    }</span><span style=''>
</span>115 <span style=''>  }
</span>116 <span style=''>
</span>117 <span style=''>  override def eval(input: InternalRow = null): Any = {
</span>118 <span style=''>    val hash = </span><span style='background: #F0ADAD'>factory.fresh</span><span style=''>
</span>119 <span style=''>    var i = </span><span style='background: #F0ADAD'>0</span><span style=''>
</span>120 <span style=''>    val len = </span><span style='background: #F0ADAD'>children.length</span><span style=''>
</span>121 <span style=''>    while (</span><span style='background: #F0ADAD'>i &lt; len</span><span style=''>) </span><span style='background: #F0ADAD'>{
</span>122 <span style=''></span><span style='background: #F0ADAD'>      computeHash(children(i).eval(input), children(i).dataType, hash)
</span>123 <span style=''></span><span style='background: #F0ADAD'>      i += 1
</span>124 <span style=''></span><span style='background: #F0ADAD'>    }</span><span style=''>
</span>125 <span style=''>    if (</span><span style='background: #F0ADAD'>asStruct</span><span style=''>)
</span>126 <span style=''>      </span><span style='background: #F0ADAD'>InternalRow(hash.digest :_*)</span><span style=''> // make the array nested
</span>127 <span style=''>    else
</span>128 <span style=''>      </span><span style='background: #F0ADAD'>new GenericArrayData(hash.digest)</span><span style=''>
</span>129 <span style=''>  }
</span>130 <span style=''>
</span>131 <span style=''>  protected def computeHash(value: Any, dataType: DataType, hash: Digest): Unit
</span>132 <span style=''>/*
</span>133 <span style=''>  override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
</span>134 <span style=''>    ev.isNull = FalseLiteral
</span>135 <span style=''>
</span>136 <span style=''>    val childrenHash = children.map { child =&gt;
</span>137 <span style=''>      val childGen = child.genCode(ctx)
</span>138 <span style=''>      childGen.code + ctx.nullSafeExec(child.nullable, childGen.isNull) {
</span>139 <span style=''>        computeHash(childGen.value, child.dataType, ev.value, ctx)
</span>140 <span style=''>      }
</span>141 <span style=''>    }
</span>142 <span style=''>
</span>143 <span style=''>    val hashResultType = &quot;Long[]&quot;
</span>144 <span style=''>    val typedSeed = s&quot;org.apache.spark.sql.qualityFunctions.Digest&quot;
</span>145 <span style=''>    val codes = ctx.splitExpressionsWithCurrentInputs(
</span>146 <span style=''>      expressions = childrenHash,
</span>147 <span style=''>      funcName = &quot;computeHash&quot;,
</span>148 <span style=''>      extraArguments = Seq(), // hashResultType -&gt; ev.value , not needed
</span>149 <span style=''>      returnType = &quot;void&quot;,
</span>150 <span style=''>      makeSplitFunction = body =&gt;
</span>151 <span style=''>        s&quot;&quot;&quot;
</span>152 <span style=''>           |$body
</span>153 <span style=''>         &quot;&quot;&quot;.stripMargin,
</span>154 <span style=''>      foldFunctions = _.map(funcCall =&gt; s&quot;${ev.value} = $funcCall;&quot;).mkString(&quot;\n&quot;))
</span>155 <span style=''>
</span>156 <span style=''>    ev.copy(code =
</span>157 <span style=''>      code&quot;&quot;&quot;
</span>158 <span style=''>            |$hashResultType ${ev.value} = $typedSeed;
</span>159 <span style=''>            |$codes
</span>160 <span style=''>       &quot;&quot;&quot;.stripMargin)
</span>161 <span style=''>  }
</span>162 <span style=''>*/
</span>163 <span style=''>  protected def nullSafeElementHash(
</span>164 <span style=''>                                     input: String,
</span>165 <span style=''>                                     index: String,
</span>166 <span style=''>                                     nullable: Boolean,
</span>167 <span style=''>                                     elementType: DataType,
</span>168 <span style=''>                                     result: String,
</span>169 <span style=''>                                     ctx: CodegenContext): String = {
</span>170 <span style=''>    val element = </span><span style='background: #F0ADAD'>ctx.freshName(&quot;element&quot;)</span><span style=''>
</span>171 <span style=''>
</span>172 <span style=''>    val jt = </span><span style='background: #F0ADAD'>CodeGenerator.javaType(elementType)</span><span style=''>
</span>173 <span style=''>    </span><span style='background: #F0ADAD'>ctx.nullSafeExec(nullable, s&quot;$input.isNullAt($index)&quot;) {
</span>174 <span style=''></span><span style='background: #F0ADAD'>      s&quot;&quot;&quot;
</span>175 <span style=''></span><span style='background: #F0ADAD'>        final $jt $element = ${CodeGenerator.getValue(input, elementType, index)};
</span>176 <span style=''></span><span style='background: #F0ADAD'>        ${computeHash(element, elementType, result, ctx)}
</span>177 <span style=''></span><span style='background: #F0ADAD'>      &quot;&quot;&quot;
</span>178 <span style=''></span><span style='background: #F0ADAD'>    }</span><span style=''>
</span>179 <span style=''>  }
</span>180 <span style=''>
</span>181 <span style=''>  protected def genHashInt(i: String, result: String): String =
</span>182 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;$result = $hasherClassName.hashInt($i, $result);&quot;</span><span style=''>
</span>183 <span style=''>
</span>184 <span style=''>  protected def genHashLong(l: String, result: String): String =
</span>185 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;$result = $hasherClassName.hashLong($l, $result);&quot;</span><span style=''>
</span>186 <span style=''>
</span>187 <span style=''>  protected def genHashBytes(b: String, result: String): String = {
</span>188 <span style=''>    val offset = </span><span style='background: #F0ADAD'>&quot;Platform.BYTE_ARRAY_OFFSET&quot;</span><span style=''>
</span>189 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;$result = $hasherClassName.hashUnsafeBytes($b, $offset, $b.length, $result);&quot;</span><span style=''>
</span>190 <span style=''>  }
</span>191 <span style=''>
</span>192 <span style=''>  protected def genHashBoolean(input: String, result: String): String =
</span>193 <span style=''>    </span><span style='background: #F0ADAD'>genHashInt(s&quot;$input ? 1 : 0&quot;, result)</span><span style=''>
</span>194 <span style=''>
</span>195 <span style=''>  protected def genHashFloat(input: String, result: String): String = {
</span>196 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;&quot;&quot;
</span>197 <span style=''></span><span style='background: #F0ADAD'>       |if($input == -0.0f) {
</span>198 <span style=''></span><span style='background: #F0ADAD'>       |  ${genHashInt(&quot;0&quot;, result)}
</span>199 <span style=''></span><span style='background: #F0ADAD'>       |} else {
</span>200 <span style=''></span><span style='background: #F0ADAD'>       |  ${genHashInt(s&quot;Float.floatToIntBits($input)&quot;, result)}
</span>201 <span style=''></span><span style='background: #F0ADAD'>       |}
</span>202 <span style=''></span><span style='background: #F0ADAD'>     &quot;&quot;&quot;.stripMargin</span><span style=''>
</span>203 <span style=''>  }
</span>204 <span style=''>
</span>205 <span style=''>  protected def genHashDouble(input: String, result: String): String = {
</span>206 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;&quot;&quot;
</span>207 <span style=''></span><span style='background: #F0ADAD'>       |if($input == -0.0d) {
</span>208 <span style=''></span><span style='background: #F0ADAD'>       |  ${genHashLong(&quot;0L&quot;, result)}
</span>209 <span style=''></span><span style='background: #F0ADAD'>       |} else {
</span>210 <span style=''></span><span style='background: #F0ADAD'>       |  ${genHashLong(s&quot;Double.doubleToLongBits($input)&quot;, result)}
</span>211 <span style=''></span><span style='background: #F0ADAD'>       |}
</span>212 <span style=''></span><span style='background: #F0ADAD'>     &quot;&quot;&quot;.stripMargin</span><span style=''>
</span>213 <span style=''>  }
</span>214 <span style=''>
</span>215 <span style=''>  protected def genHashDecimal(
</span>216 <span style=''>                                ctx: CodegenContext,
</span>217 <span style=''>                                d: DecimalType,
</span>218 <span style=''>                                input: String,
</span>219 <span style=''>                                result: String): String = {
</span>220 <span style=''>    if (</span><span style='background: #F0ADAD'>d.precision &lt;= Decimal.MAX_LONG_DIGITS</span><span style=''>) {
</span>221 <span style=''>      </span><span style='background: #F0ADAD'>genHashLong(s&quot;$input.toUnscaledLong()&quot;, result)</span><span style=''>
</span>222 <span style=''>    } else </span><span style='background: #F0ADAD'>{
</span>223 <span style=''></span><span style='background: #F0ADAD'>      val bytes = ctx.freshName(&quot;bytes&quot;)
</span>224 <span style=''></span><span style='background: #F0ADAD'>      s&quot;&quot;&quot;
</span>225 <span style=''></span><span style='background: #F0ADAD'>         |final byte[] $bytes = $input.toJavaBigDecimal().unscaledValue().toByteArray();
</span>226 <span style=''></span><span style='background: #F0ADAD'>         |${genHashBytes(bytes, result)}
</span>227 <span style=''></span><span style='background: #F0ADAD'>       &quot;&quot;&quot;.stripMargin
</span>228 <span style=''></span><span style='background: #F0ADAD'>    }</span><span style=''>
</span>229 <span style=''>  }
</span>230 <span style=''>
</span>231 <span style=''>  protected def genHashTimestamp(t: String, result: String): String = </span><span style='background: #F0ADAD'>genHashLong(t, result)</span><span style=''>
</span>232 <span style=''>
</span>233 <span style=''>  protected def genHashCalendarInterval(input: String, result: String): String = {
</span>234 <span style=''>    val microsecondsHash = </span><span style='background: #F0ADAD'>s&quot;$hasherClassName.hashLong($input.microseconds, $result)&quot;</span><span style=''>
</span>235 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;$result = $hasherClassName.hashInt($input.months, $microsecondsHash);&quot;</span><span style=''>
</span>236 <span style=''>  }
</span>237 <span style=''>
</span>238 <span style=''>  protected def genHashString(input: String, result: String): String = {
</span>239 <span style=''>    val baseObject = </span><span style='background: #F0ADAD'>s&quot;$input.getBaseObject()&quot;</span><span style=''>
</span>240 <span style=''>    val baseOffset = </span><span style='background: #F0ADAD'>s&quot;$input.getBaseOffset()&quot;</span><span style=''>
</span>241 <span style=''>    val numBytes = </span><span style='background: #F0ADAD'>s&quot;$input.numBytes()&quot;</span><span style=''>
</span>242 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;$result = $hasherClassName.hashUnsafeBytes($baseObject, $baseOffset, $numBytes, $result);&quot;</span><span style=''>
</span>243 <span style=''>  }
</span>244 <span style=''>
</span>245 <span style=''>  protected def genHashForMap(
</span>246 <span style=''>                               ctx: CodegenContext,
</span>247 <span style=''>                               input: String,
</span>248 <span style=''>                               result: String,
</span>249 <span style=''>                               keyType: DataType,
</span>250 <span style=''>                               valueType: DataType,
</span>251 <span style=''>                               valueContainsNull: Boolean): String = {
</span>252 <span style=''>    val index = </span><span style='background: #F0ADAD'>ctx.freshName(&quot;index&quot;)</span><span style=''>
</span>253 <span style=''>    val keys = </span><span style='background: #F0ADAD'>ctx.freshName(&quot;keys&quot;)</span><span style=''>
</span>254 <span style=''>    val values = </span><span style='background: #F0ADAD'>ctx.freshName(&quot;values&quot;)</span><span style=''>
</span>255 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;&quot;&quot;
</span>256 <span style=''></span><span style='background: #F0ADAD'>        final ArrayData $keys = $input.keyArray();
</span>257 <span style=''></span><span style='background: #F0ADAD'>        final ArrayData $values = $input.valueArray();
</span>258 <span style=''></span><span style='background: #F0ADAD'>        for (int $index = 0; $index &lt; $input.numElements(); $index++) {
</span>259 <span style=''></span><span style='background: #F0ADAD'>          ${nullSafeElementHash(keys, index, false, keyType, result, ctx)}
</span>260 <span style=''></span><span style='background: #F0ADAD'>          ${nullSafeElementHash(values, index, valueContainsNull, valueType, result, ctx)}
</span>261 <span style=''></span><span style='background: #F0ADAD'>        }
</span>262 <span style=''></span><span style='background: #F0ADAD'>      &quot;&quot;&quot;</span><span style=''>
</span>263 <span style=''>  }
</span>264 <span style=''>
</span>265 <span style=''>  protected def genHashForArray(
</span>266 <span style=''>                                 ctx: CodegenContext,
</span>267 <span style=''>                                 input: String,
</span>268 <span style=''>                                 result: String,
</span>269 <span style=''>                                 elementType: DataType,
</span>270 <span style=''>                                 containsNull: Boolean): String = {
</span>271 <span style=''>    val index = </span><span style='background: #F0ADAD'>ctx.freshName(&quot;index&quot;)</span><span style=''>
</span>272 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;&quot;&quot;
</span>273 <span style=''></span><span style='background: #F0ADAD'>        for (int $index = 0; $index &lt; $input.numElements(); $index++) {
</span>274 <span style=''></span><span style='background: #F0ADAD'>          ${nullSafeElementHash(input, index, containsNull, elementType, result, ctx)}
</span>275 <span style=''></span><span style='background: #F0ADAD'>        }
</span>276 <span style=''></span><span style='background: #F0ADAD'>      &quot;&quot;&quot;</span><span style=''>
</span>277 <span style=''>  }
</span>278 <span style=''>
</span>279 <span style=''>  protected def genHashForStruct(
</span>280 <span style=''>                                  ctx: CodegenContext,
</span>281 <span style=''>                                  input: String,
</span>282 <span style=''>                                  result: String,
</span>283 <span style=''>                                  fields: Array[StructField]): String = {
</span>284 <span style=''>    val tmpInput = </span><span style='background: #F0ADAD'>ctx.freshName(&quot;input&quot;)</span><span style=''>
</span>285 <span style=''>    val fieldsHash = </span><span style='background: #F0ADAD'>fields.zipWithIndex.map { case (field, index) =&gt;
</span>286 <span style=''></span><span style='background: #F0ADAD'>      nullSafeElementHash(tmpInput, index.toString, field.nullable, field.dataType, result, ctx)
</span>287 <span style=''></span><span style='background: #F0ADAD'>    }</span><span style=''>
</span>288 <span style=''>    val hashResultType = </span><span style='background: #F0ADAD'>CodeGenerator.javaType(dataType)</span><span style=''>
</span>289 <span style=''>    val code = </span><span style='background: #F0ADAD'>ctx.splitExpressions(
</span>290 <span style=''></span><span style='background: #F0ADAD'>      expressions = fieldsHash,
</span>291 <span style=''></span><span style='background: #F0ADAD'>      funcName = &quot;computeHashForStruct&quot;,
</span>292 <span style=''></span><span style='background: #F0ADAD'>      arguments = Seq(&quot;InternalRow&quot; -&gt; tmpInput, hashResultType -&gt; result),
</span>293 <span style=''></span><span style='background: #F0ADAD'>      returnType = hashResultType,
</span>294 <span style=''></span><span style='background: #F0ADAD'>      makeSplitFunction = body =&gt;
</span>295 <span style=''></span><span style='background: #F0ADAD'>        s&quot;&quot;&quot;
</span>296 <span style=''></span><span style='background: #F0ADAD'>           |$body
</span>297 <span style=''></span><span style='background: #F0ADAD'>           |return $result;
</span>298 <span style=''></span><span style='background: #F0ADAD'>         &quot;&quot;&quot;.stripMargin,
</span>299 <span style=''></span><span style='background: #F0ADAD'>      foldFunctions = _.map(funcCall =&gt; s&quot;$result = $funcCall;&quot;).mkString(&quot;\n&quot;))</span><span style=''>
</span>300 <span style=''>    </span><span style='background: #F0ADAD'>s&quot;&quot;&quot;
</span>301 <span style=''></span><span style='background: #F0ADAD'>       |final InternalRow $tmpInput = $input;
</span>302 <span style=''></span><span style='background: #F0ADAD'>       |$code
</span>303 <span style=''></span><span style='background: #F0ADAD'>     &quot;&quot;&quot;.stripMargin</span><span style=''>
</span>304 <span style=''>  }
</span>305 <span style=''>
</span>306 <span style=''>  @tailrec
</span>307 <span style=''>  private def computeHashWithTailRec(
</span>308 <span style=''>                                      input: String,
</span>309 <span style=''>                                      dataType: DataType,
</span>310 <span style=''>                                      result: String,
</span>311 <span style=''>                                      ctx: CodegenContext): String = dataType match {
</span>312 <span style=''>    case NullType =&gt; </span><span style='background: #F0ADAD'>&quot;&quot;</span><span style=''>
</span>313 <span style=''>    case BooleanType =&gt; </span><span style='background: #F0ADAD'>genHashBoolean(input, result)</span><span style=''>
</span>314 <span style=''>    case ByteType | ShortType | IntegerType | DateType =&gt; </span><span style='background: #F0ADAD'>genHashInt(input, result)</span><span style=''>
</span>315 <span style=''>    case LongType =&gt; </span><span style='background: #F0ADAD'>genHashLong(input, result)</span><span style=''>
</span>316 <span style=''>    case TimestampType =&gt; </span><span style='background: #F0ADAD'>genHashTimestamp(input, result)</span><span style=''>
</span>317 <span style=''>    case FloatType =&gt; </span><span style='background: #F0ADAD'>genHashFloat(input, result)</span><span style=''>
</span>318 <span style=''>    case DoubleType =&gt; </span><span style='background: #F0ADAD'>genHashDouble(input, result)</span><span style=''>
</span>319 <span style=''>    case d: DecimalType =&gt; </span><span style='background: #F0ADAD'>genHashDecimal(ctx, d, input, result)</span><span style=''>
</span>320 <span style=''>    case CalendarIntervalType =&gt; </span><span style='background: #F0ADAD'>genHashCalendarInterval(input, result)</span><span style=''>
</span>321 <span style=''>      // TODO figure these out
</span>322 <span style=''>//    case _: DayTimeIntervalType =&gt; genHashLong(input, result)
</span>323 <span style=''>//    case YearMonthIntervalType =&gt; genHashInt(input, result)
</span>324 <span style=''>    case BinaryType =&gt; </span><span style='background: #F0ADAD'>genHashBytes(input, result)</span><span style=''>
</span>325 <span style=''>    case StringType =&gt; </span><span style='background: #F0ADAD'>genHashString(input, result)</span><span style=''>
</span>326 <span style=''>    case ArrayType(et, containsNull) =&gt; </span><span style='background: #F0ADAD'>genHashForArray(ctx, input, result, et, containsNull)</span><span style=''>
</span>327 <span style=''>    case MapType(kt, vt, valueContainsNull) =&gt;
</span>328 <span style=''>      </span><span style='background: #F0ADAD'>genHashForMap(ctx, input, result, kt, vt, valueContainsNull)</span><span style=''>
</span>329 <span style=''>    case StructType(fields) =&gt; </span><span style='background: #F0ADAD'>genHashForStruct(ctx, input, result, fields)</span><span style=''>
</span>330 <span style=''>    case udt: UserDefinedType[_] =&gt; </span><span style='background: #F0ADAD'>computeHashWithTailRec(input, udt.sqlType, result, ctx)</span><span style=''>
</span>331 <span style=''>  }
</span>332 <span style=''>
</span>333 <span style=''>  protected def computeHash(
</span>334 <span style=''>                             input: String,
</span>335 <span style=''>                             dataType: DataType,
</span>336 <span style=''>                             result: String,
</span>337 <span style=''>                             ctx: CodegenContext): String = </span><span style='background: #F0ADAD'>computeHashWithTailRec(input, dataType, result, ctx)</span><span style=''>
</span>338 <span style=''>
</span>339 <span style=''>  protected def hasherClassName: String
</span>340 <span style=''>}
</span>341 <span style=''>
</span>342 <span style=''>object SafeUTF8 {
</span>343 <span style=''>  /**
</span>344 <span style=''>   * Returns the actual byte array if it's a byte array, otherwise gets the bytes serialisation of it
</span>345 <span style=''>   * @param s
</span>346 <span style=''>   * @return
</span>347 <span style=''>   */
</span>348 <span style=''>  def safeUT8ByteArray(s: UTF8String): (Array[Byte], Int, Int) = {
</span>349 <span style=''>    if (</span><span style='background: #F0ADAD'>s.getBaseObject.isInstanceOf[Array[Byte]] &amp;&amp; s.getBaseOffset &gt;= Platform.BYTE_ARRAY_OFFSET.toLong</span><span style=''>) </span><span style='background: #F0ADAD'>{
</span>350 <span style=''></span><span style='background: #F0ADAD'>      val bytes = s.getBaseObject.asInstanceOf[Array[Byte]].asInstanceOf[Array[Byte]]
</span>351 <span style=''></span><span style='background: #F0ADAD'>      val arrayOffset = s.getBaseOffset - Platform.BYTE_ARRAY_OFFSET.toLong
</span>352 <span style=''></span><span style='background: #F0ADAD'>      if (bytes.length.toLong &lt; arrayOffset + s.numBytes.toLong) throw new ArrayIndexOutOfBoundsException
</span>353 <span style=''></span><span style='background: #F0ADAD'>      else
</span>354 <span style=''></span><span style='background: #F0ADAD'>        (bytes, arrayOffset.toInt, s.numBytes)
</span>355 <span style=''></span><span style='background: #F0ADAD'>    }</span><span style=''>
</span>356 <span style=''>    else {
</span>357 <span style=''>      </span><span style='background: #F0ADAD'>(s.getBytes, 0, s.numBytes)</span><span style=''>
</span>358 <span style=''>    }
</span>359 <span style=''>  }
</span>360 <span style=''>}
</span>361 <span style=''>
</span>362 <span style=''>/**
</span>363 <span style=''> * Base class for interpreted hash functions.
</span>364 <span style=''> */
</span>365 <span style=''>abstract class InterpretedHashLongsFunction {
</span>366 <span style=''>  def hashInt(i: Int, digest: Digest): Digest
</span>367 <span style=''>
</span>368 <span style=''>  def hashLong(l: Long, digest: Digest): Digest
</span>369 <span style=''>
</span>370 <span style=''>  def hashBytes(base: Array[Byte], offset: Int, length: Int, digest: Digest): Digest
</span>371 <span style=''>
</span>372 <span style=''>  /**
</span>373 <span style=''>   * Computes hash of a given `value` of type `dataType`. The caller needs to check the validity
</span>374 <span style=''>   * of input `value`.
</span>375 <span style=''>   */
</span>376 <span style=''>  def hash(value: Any, dataType: DataType, digest: Digest): Digest = {
</span>377 <span style=''>    value match {
</span>378 <span style=''>      case null =&gt; digest
</span>379 <span style=''>      case b: Boolean =&gt; </span><span style='background: #F0ADAD'>hashInt(if (b) 1 else 0, digest)</span><span style=''>
</span>380 <span style=''>      case b: Byte =&gt; </span><span style='background: #F0ADAD'>hashInt(b, digest)</span><span style=''>
</span>381 <span style=''>      case s: Short =&gt; </span><span style='background: #F0ADAD'>hashInt(s, digest)</span><span style=''>
</span>382 <span style=''>      case i: Int =&gt; </span><span style='background: #F0ADAD'>hashInt(i, digest)</span><span style=''>
</span>383 <span style=''>      case l: Long =&gt; </span><span style='background: #F0ADAD'>hashLong(l, digest)</span><span style=''>
</span>384 <span style=''>      case f: Float if (</span><span style='background: #F0ADAD'>f == -0.0f</span><span style=''>) =&gt; </span><span style='background: #F0ADAD'>hashInt(0, digest)</span><span style=''>
</span>385 <span style=''>      case f: Float =&gt; </span><span style='background: #F0ADAD'>hashInt(java.lang.Float.floatToIntBits(f), digest)</span><span style=''>
</span>386 <span style=''>      case d: Double if (</span><span style='background: #F0ADAD'>d == -0.0d</span><span style=''>) =&gt; </span><span style='background: #F0ADAD'>hashLong(0L, digest)</span><span style=''>
</span>387 <span style=''>      case d: Double =&gt; </span><span style='background: #F0ADAD'>hashLong(java.lang.Double.doubleToLongBits(d), digest)</span><span style=''>
</span>388 <span style=''>      case d: Decimal =&gt;
</span>389 <span style=''>        val precision = </span><span style='background: #F0ADAD'>dataType.asInstanceOf[DecimalType].precision</span><span style=''>
</span>390 <span style=''>        if (</span><span style='background: #F0ADAD'>precision &lt;= Decimal.MAX_LONG_DIGITS</span><span style=''>) {
</span>391 <span style=''>          </span><span style='background: #F0ADAD'>hashLong(d.toUnscaledLong, digest)</span><span style=''>
</span>392 <span style=''>        } else </span><span style='background: #F0ADAD'>{
</span>393 <span style=''></span><span style='background: #F0ADAD'>          val bytes = d.toJavaBigDecimal.unscaledValue().toByteArray
</span>394 <span style=''></span><span style='background: #F0ADAD'>          hashBytes(bytes, Platform.BYTE_ARRAY_OFFSET, bytes.length, digest)
</span>395 <span style=''></span><span style='background: #F0ADAD'>        }</span><span style=''>
</span>396 <span style=''>      case c: CalendarInterval =&gt;
</span>397 <span style=''>        </span><span style='background: #F0ADAD'>ShimUtils.hashCalendarInterval(c, this, digest)</span><span style=''>
</span>398 <span style=''>      case a: Array[Byte] =&gt;
</span>399 <span style=''>        </span><span style='background: #F0ADAD'>hashBytes(a, Platform.BYTE_ARRAY_OFFSET, a.length, digest)</span><span style=''>
</span>400 <span style=''>      case s: UTF8String =&gt; {
</span>401 <span style=''>        /* */
</span>402 <span style=''>        val (bytes, offset, numbytes) = SafeUTF8.safeUT8ByteArray(s)
</span>403 <span style=''>
</span>404 <span style=''>        </span><span style='background: #F0ADAD'>hashBytes(bytes, offset, numbytes, digest)</span><span style=''>
</span>405 <span style=''>
</span>406 <span style=''>      }
</span>407 <span style=''>
</span>408 <span style=''>      case array: ArrayData =&gt;
</span>409 <span style=''>        val elementType = dataType match {
</span>410 <span style=''>          case udt: UserDefinedType[_] =&gt; </span><span style='background: #F0ADAD'>udt.sqlType.asInstanceOf[ArrayType].elementType</span><span style=''>
</span>411 <span style=''>          case ArrayType(et, _) =&gt; et
</span>412 <span style=''>        }
</span>413 <span style=''>        var result = digest
</span>414 <span style=''>        var i = </span><span style='background: #F0ADAD'>0</span><span style=''>
</span>415 <span style=''>        while (</span><span style='background: #F0ADAD'>i &lt; array.numElements()</span><span style=''>) </span><span style='background: #F0ADAD'>{
</span>416 <span style=''></span><span style='background: #F0ADAD'>          result = hash(array.get(i, elementType), elementType, result)
</span>417 <span style=''></span><span style='background: #F0ADAD'>          i += 1
</span>418 <span style=''></span><span style='background: #F0ADAD'>        }</span><span style=''>
</span>419 <span style=''>        result
</span>420 <span style=''>
</span>421 <span style=''>      case map: MapData =&gt;
</span>422 <span style=''>        val (kt, vt) = dataType match {
</span>423 <span style=''>          case udt: UserDefinedType[_] =&gt;
</span>424 <span style=''>            val mapType = udt.sqlType.asInstanceOf[MapType]
</span>425 <span style=''>            mapType.keyType -&gt; mapType.valueType
</span>426 <span style=''>          case MapType(kt, vt, _) =&gt; kt -&gt; vt
</span>427 <span style=''>        }
</span>428 <span style=''>        val keys = </span><span style='background: #F0ADAD'>map.keyArray()</span><span style=''>
</span>429 <span style=''>        val values = </span><span style='background: #F0ADAD'>map.valueArray()</span><span style=''>
</span>430 <span style=''>        var result = digest
</span>431 <span style=''>        var i = </span><span style='background: #F0ADAD'>0</span><span style=''>
</span>432 <span style=''>        while (</span><span style='background: #F0ADAD'>i &lt; map.numElements()</span><span style=''>) </span><span style='background: #F0ADAD'>{
</span>433 <span style=''></span><span style='background: #F0ADAD'>          result = hash(keys.get(i, kt), kt, result)
</span>434 <span style=''></span><span style='background: #F0ADAD'>          result = hash(values.get(i, vt), vt, result)
</span>435 <span style=''></span><span style='background: #F0ADAD'>          i += 1
</span>436 <span style=''></span><span style='background: #F0ADAD'>        }</span><span style=''>
</span>437 <span style=''>        result
</span>438 <span style=''>
</span>439 <span style=''>      case struct: InternalRow =&gt;
</span>440 <span style=''>        val types: Array[DataType] = dataType match {
</span>441 <span style=''>          case udt: UserDefinedType[_] =&gt;
</span>442 <span style=''>            </span><span style='background: #F0ADAD'>udt.sqlType.asInstanceOf[StructType].map(_.dataType).toArray</span><span style=''>
</span>443 <span style=''>          case StructType(fields) =&gt; </span><span style='background: #F0ADAD'>fields.map(_.dataType)</span><span style=''>
</span>444 <span style=''>        }
</span>445 <span style=''>        var result = digest
</span>446 <span style=''>        var i = </span><span style='background: #F0ADAD'>0</span><span style=''>
</span>447 <span style=''>        val len = </span><span style='background: #F0ADAD'>struct.numFields</span><span style=''>
</span>448 <span style=''>        while (</span><span style='background: #F0ADAD'>i &lt; len</span><span style=''>) </span><span style='background: #F0ADAD'>{
</span>449 <span style=''></span><span style='background: #F0ADAD'>          result = hash(struct.get(i, types(i)), types(i), result)
</span>450 <span style=''></span><span style='background: #F0ADAD'>          i += 1
</span>451 <span style=''></span><span style='background: #F0ADAD'>        }</span><span style=''>
</span>452 <span style=''>        result
</span>453 <span style=''>    }
</span>454 <span style=''>  }
</span>455 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Tests</th>
        <th>Code</th>
      </tr><tr>
        <td>
          78
        </td>
        <td>
          190
        </td>
        <td>
          3541
          -
          3549
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.asStruct
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.asStruct
        </td>
      </tr><tr>
        <td>
          79
        </td>
        <td>
          21
        </td>
        <td>
          3557
          -
          3666
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.StructType.apply(scala.Predef.intWrapper(0).until(HashLongsExpression.this.factory.length).map[org.apache.spark.sql.types.StructField, scala.collection.immutable.IndexedSeq[org.apache.spark.sql.types.StructField]](((i: Int) =&gt; org.apache.spark.sql.types.StructField.apply(&quot;i&quot;.+(i), org.apache.spark.sql.types.LongType, org.apache.spark.sql.types.StructField.apply$default$3, org.apache.spark.sql.types.StructField.apply$default$4)))(immutable.this.IndexedSeq.canBuildFrom[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          79
        </td>
        <td>
          416
        </td>
        <td>
          3557
          -
          3666
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.types.StructType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.StructType.apply(scala.Predef.intWrapper(0).until(HashLongsExpression.this.factory.length).map[org.apache.spark.sql.types.StructField, scala.collection.immutable.IndexedSeq[org.apache.spark.sql.types.StructField]](((i: Int) =&gt; org.apache.spark.sql.types.StructField.apply(&quot;i&quot;.+(i), org.apache.spark.sql.types.LongType, org.apache.spark.sql.types.StructField.apply$default$3, org.apache.spark.sql.types.StructField.apply$default$4)))(immutable.this.IndexedSeq.canBuildFrom[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          32
        </td>
        <td>
          3648
          -
          3656
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          403
        </td>
        <td>
          3586
          -
          3600
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.shim.hash.DigestFactory.length
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.factory.length
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          8
        </td>
        <td>
          3578
          -
          3579
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          340
        </td>
        <td>
          3611
          -
          3611
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply$default$3
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.StructField.apply$default$3
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          193
        </td>
        <td>
          3578
          -
          3658
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.intWrapper(0).until(HashLongsExpression.this.factory.length).map[org.apache.spark.sql.types.StructField, scala.collection.immutable.IndexedSeq[org.apache.spark.sql.types.StructField]](((i: Int) =&gt; org.apache.spark.sql.types.StructField.apply(&quot;i&quot;.+(i), org.apache.spark.sql.types.LongType, org.apache.spark.sql.types.StructField.apply$default$3, org.apache.spark.sql.types.StructField.apply$default$4)))(immutable.this.IndexedSeq.canBuildFrom[org.apache.spark.sql.types.StructField])
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          237
        </td>
        <td>
          3630
          -
          3635
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;i&quot;.+(i)
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          159
        </td>
        <td>
          3611
          -
          3611
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply$default$4
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.StructField.apply$default$4
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          93
        </td>
        <td>
          3611
          -
          3657
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructField.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.StructField.apply(&quot;i&quot;.+(i), org.apache.spark.sql.types.LongType, org.apache.spark.sql.types.StructField.apply$default$3, org.apache.spark.sql.types.StructField.apply$default$4)
        </td>
      </tr><tr>
        <td>
          80
        </td>
        <td>
          380
        </td>
        <td>
          3605
          -
          3605
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.immutable.IndexedSeq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          immutable.this.IndexedSeq.canBuildFrom[org.apache.spark.sql.types.StructField]
        </td>
      </tr><tr>
        <td>
          83
        </td>
        <td>
          343
        </td>
        <td>
          3682
          -
          3701
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.types.ArrayType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.ArrayType.apply(org.apache.spark.sql.types.LongType)
        </td>
      </tr><tr>
        <td>
          83
        </td>
        <td>
          58
        </td>
        <td>
          3682
          -
          3701
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.ArrayType.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.ArrayType.apply(org.apache.spark.sql.types.LongType)
        </td>
      </tr><tr>
        <td>
          83
        </td>
        <td>
          239
        </td>
        <td>
          3692
          -
          3700
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.LongType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.LongType
        </td>
      </tr><tr>
        <td>
          85
        </td>
        <td>
          169
        </td>
        <td>
          3754
          -
          3764
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.foldable
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$1.foldable
        </td>
      </tr><tr>
        <td>
          85
        </td>
        <td>
          88
        </td>
        <td>
          3738
          -
          3765
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.forall
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.children.forall(((x$1: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; x$1.foldable))
        </td>
      </tr><tr>
        <td>
          87
        </td>
        <td>
          382
        </td>
        <td>
          3802
          -
          3807
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          false
        </td>
      </tr><tr>
        <td>
          90
        </td>
        <td>
          23
        </td>
        <td>
          3865
          -
          3910
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.DataType.existsRecursively
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          dt.existsRecursively(((x$2: org.apache.spark.sql.types.DataType) =&gt; x$2.isInstanceOf[org.apache.spark.sql.types.MapType]))
        </td>
      </tr><tr>
        <td>
          90
        </td>
        <td>
          183
        </td>
        <td>
          3886
          -
          3909
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.isInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$2.isInstanceOf[org.apache.spark.sql.types.MapType]
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          315
        </td>
        <td>
          3982
          -
          4001
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.children.length.&lt;(1)
        </td>
      </tr><tr>
        <td>
          95
        </td>
        <td>
          77
        </td>
        <td>
          4011
          -
          4117
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;input to function &quot;, &quot; requires at least one argument&quot;).s(HashLongsExpression.this.prettyName))
        </td>
      </tr><tr>
        <td>
          95
        </td>
        <td>
          371
        </td>
        <td>
          4011
          -
          4117
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;input to function &quot;, &quot; requires at least one argument&quot;).s(HashLongsExpression.this.prettyName))
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          229
        </td>
        <td>
          4055
          -
          4074
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;input to function &quot;
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          37
        </td>
        <td>
          4084
          -
          4116
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; requires at least one argument&quot;
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          156
        </td>
        <td>
          4053
          -
          4116
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;input to function &quot;, &quot; requires at least one argument&quot;).s(HashLongsExpression.this.prettyName)
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          344
        </td>
        <td>
          4074
          -
          4084
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.prettyName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.prettyName
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          379
        </td>
        <td>
          4693
          -
          5050
        </td>
        <td>
          If
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          if (HashLongsExpression.this.children.exists(((child: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; HashLongsExpression.this.hasMapType(child.dataType))))
  org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;input to function &quot;, &quot; cannot contain elements of MapType. In Spark, same maps &quot;).s(HashLongsExpression.this.prettyName).+(&quot;may have different hashcode, thus hash expressions are prohibited on MapType elements.&quot;))
else
  org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckSuccess
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          184
        </td>
        <td>
          4733
          -
          4747
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          child.dataType
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          309
        </td>
        <td>
          4697
          -
          4749
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.IterableLike.exists
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.children.exists(((child: org.apache.spark.sql.catalyst.expressions.Expression) =&gt; HashLongsExpression.this.hasMapType(child.dataType)))
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          24
        </td>
        <td>
          4722
          -
          4748
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.hasMapType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.hasMapType(child.dataType)
        </td>
      </tr><tr>
        <td>
          109
        </td>
        <td>
          336
        </td>
        <td>
          4759
          -
          4992
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;input to function &quot;, &quot; cannot contain elements of MapType. In Spark, same maps &quot;).s(HashLongsExpression.this.prettyName).+(&quot;may have different hashcode, thus hash expressions are prohibited on MapType elements.&quot;))
        </td>
      </tr><tr>
        <td>
          109
        </td>
        <td>
          34
        </td>
        <td>
          4759
          -
          4992
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckFailure.apply(scala.StringContext.apply(&quot;input to function &quot;, &quot; cannot contain elements of MapType. In Spark, same maps &quot;).s(HashLongsExpression.this.prettyName).+(&quot;may have different hashcode, thus hash expressions are prohibited on MapType elements.&quot;))
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          220
        </td>
        <td>
          4801
          -
          4991
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;input to function &quot;, &quot; cannot contain elements of MapType. In Spark, same maps &quot;).s(HashLongsExpression.this.prettyName).+(&quot;may have different hashcode, thus hash expressions are prohibited on MapType elements.&quot;)
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          158
        </td>
        <td>
          5012
          -
          5044
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckSuccess
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckSuccess
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          67
        </td>
        <td>
          5012
          -
          5044
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckSuccess
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.analysis.TypeCheckResult.TypeCheckSuccess
        </td>
      </tr><tr>
        <td>
          118
        </td>
        <td>
          185
        </td>
        <td>
          5127
          -
          5140
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.shim.hash.DigestFactory.fresh
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.factory.fresh
        </td>
      </tr><tr>
        <td>
          119
        </td>
        <td>
          1
        </td>
        <td>
          5153
          -
          5154
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          120
        </td>
        <td>
          311
        </td>
        <td>
          5169
          -
          5184
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.SeqLike.length
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.children.length
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          2
        </td>
        <td>
          5189
          -
          5189
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          301
        </td>
        <td>
          5189
          -
          5189
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          213
        </td>
        <td>
          5196
          -
          5203
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          i.&lt;(len)
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          368
        </td>
        <td>
          5205
          -
          5205
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.while$1
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          while$1()
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          203
        </td>
        <td>
          5205
          -
          5296
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  {
    HashLongsExpression.this.computeHash(HashLongsExpression.this.children.apply(i).eval(input), HashLongsExpression.this.children.apply(i).dataType, hash);
    i = i.+(1)
  };
  while$1()
}
        </td>
      </tr><tr>
        <td>
          122
        </td>
        <td>
          149
        </td>
        <td>
          5213
          -
          5277
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.computeHash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.computeHash(HashLongsExpression.this.children.apply(i).eval(input), HashLongsExpression.this.children.apply(i).dataType, hash)
        </td>
      </tr><tr>
        <td>
          122
        </td>
        <td>
          337
        </td>
        <td>
          5250
          -
          5270
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.children.apply(i).dataType
        </td>
      </tr><tr>
        <td>
          122
        </td>
        <td>
          57
        </td>
        <td>
          5225
          -
          5248
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.Expression.eval
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.children.apply(i).eval(input)
        </td>
      </tr><tr>
        <td>
          123
        </td>
        <td>
          453
        </td>
        <td>
          5284
          -
          5290
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          i.+(1)
        </td>
      </tr><tr>
        <td>
          125
        </td>
        <td>
          227
        </td>
        <td>
          5305
          -
          5313
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.asStruct
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.asStruct
        </td>
      </tr><tr>
        <td>
          126
        </td>
        <td>
          59
        </td>
        <td>
          5333
          -
          5344
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.shim.hash.Digest.digest
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          hash.digest
        </td>
      </tr><tr>
        <td>
          126
        </td>
        <td>
          334
        </td>
        <td>
          5321
          -
          5349
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.InternalRow.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.InternalRow.apply((hash.digest: _*))
        </td>
      </tr><tr>
        <td>
          126
        </td>
        <td>
          150
        </td>
        <td>
          5321
          -
          5349
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.catalyst.InternalRow.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.InternalRow.apply((hash.digest: _*))
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          448
        </td>
        <td>
          5411
          -
          5422
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.shim.hash.Digest.digest
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          hash.digest
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          370
        </td>
        <td>
          5390
          -
          5423
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.GenericArrayData.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          new org.apache.spark.sql.catalyst.util.GenericArrayData(hash.digest)
        </td>
      </tr><tr>
        <td>
          128
        </td>
        <td>
          204
        </td>
        <td>
          5390
          -
          5423
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.GenericArrayData.&lt;init&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          new org.apache.spark.sql.catalyst.util.GenericArrayData(hash.digest)
        </td>
      </tr><tr>
        <td>
          170
        </td>
        <td>
          17
        </td>
        <td>
          6896
          -
          6920
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.freshName(&quot;element&quot;)
        </td>
      </tr><tr>
        <td>
          172
        </td>
        <td>
          303
        </td>
        <td>
          6935
          -
          6970
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType(elementType)
        </td>
      </tr><tr>
        <td>
          173
        </td>
        <td>
          138
        </td>
        <td>
          6975
          -
          7199
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.nullSafeExec
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.nullSafeExec(nullable, scala.StringContext.apply(&quot;&quot;, &quot;.isNullAt(&quot;, &quot;)&quot;).s(input, index))(scala.StringContext.apply(&quot;\n        final &quot;, &quot; &quot;, &quot; = &quot;, &quot;;\n        &quot;, &quot;\n      &quot;).s(jt, element, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.getValue(input, elementType, index), HashLongsExpression.this.computeHash(element, elementType, result, ctx)))
        </td>
      </tr><tr>
        <td>
          173
        </td>
        <td>
          230
        </td>
        <td>
          7002
          -
          7028
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;.isNullAt(&quot;, &quot;)&quot;).s(input, index)
        </td>
      </tr><tr>
        <td>
          174
        </td>
        <td>
          316
        </td>
        <td>
          7038
          -
          7193
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n        final &quot;, &quot; &quot;, &quot; = &quot;, &quot;;\n        &quot;, &quot;\n      &quot;).s(jt, element, org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.getValue(input, elementType, index), HashLongsExpression.this.computeHash(element, elementType, result, ctx))
        </td>
      </tr><tr>
        <td>
          174
        </td>
        <td>
          45
        </td>
        <td>
          7042
          -
          7058
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n        final &quot;
        </td>
      </tr><tr>
        <td>
          175
        </td>
        <td>
          164
        </td>
        <td>
          7069
          -
          7073
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          175
        </td>
        <td>
          457
        </td>
        <td>
          7124
          -
          7135
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;;\n        &quot;
        </td>
      </tr><tr>
        <td>
          175
        </td>
        <td>
          177
        </td>
        <td>
          7074
          -
          7123
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.getValue
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.getValue(input, elementType, index)
        </td>
      </tr><tr>
        <td>
          175
        </td>
        <td>
          335
        </td>
        <td>
          7060
          -
          7062
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; &quot;
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          372
        </td>
        <td>
          7183
          -
          7193
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n      &quot;
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          18
        </td>
        <td>
          7136
          -
          7182
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.computeHash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.computeHash(element, elementType, result, ctx)
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          364
        </td>
        <td>
          7321
          -
          7324
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;);&quot;
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          178
        </td>
        <td>
          7286
          -
          7301
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.hasherClassName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.hasherClassName
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          46
        </td>
        <td>
          7275
          -
          7276
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          10
        </td>
        <td>
          7273
          -
          7324
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; = &quot;, &quot;.hashInt(&quot;, &quot;, &quot;, &quot;);&quot;).s(result, HashLongsExpression.this.hasherClassName, i, result)
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          452
        </td>
        <td>
          7312
          -
          7315
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, &quot;
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          165
        </td>
        <td>
          7301
          -
          7311
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.hashInt(&quot;
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          329
        </td>
        <td>
          7282
          -
          7286
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          166
        </td>
        <td>
          7444
          -
          7447
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;);&quot;
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          447
        </td>
        <td>
          7408
          -
          7423
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.hasherClassName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.hasherClassName
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          300
        </td>
        <td>
          7397
          -
          7398
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          330
        </td>
        <td>
          7435
          -
          7438
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, &quot;
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          279
        </td>
        <td>
          7395
          -
          7447
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; = &quot;, &quot;.hashLong(&quot;, &quot;, &quot;, &quot;);&quot;).s(result, HashLongsExpression.this.hasherClassName, l, result)
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          132
        </td>
        <td>
          7404
          -
          7408
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          39
        </td>
        <td>
          7423
          -
          7434
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.hashLong(&quot;
        </td>
      </tr><tr>
        <td>
          188
        </td>
        <td>
          176
        </td>
        <td>
          7534
          -
          7562
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;Platform.BYTE_ARRAY_OFFSET&quot;
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          128
        </td>
        <td>
          7595
          -
          7613
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.hashUnsafeBytes(&quot;
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          44
        </td>
        <td>
          7614
          -
          7617
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, &quot;
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          196
        </td>
        <td>
          7567
          -
          7646
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; = &quot;, &quot;.hashUnsafeBytes(&quot;, &quot;, &quot;, &quot;, &quot;, &quot;.length, &quot;, &quot;);&quot;).s(result, HashLongsExpression.this.hasherClassName, b, offset, b, result)
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          449
        </td>
        <td>
          7643
          -
          7646
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;);&quot;
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          323
        </td>
        <td>
          7623
          -
          7626
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, &quot;
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          12
        </td>
        <td>
          7569
          -
          7570
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          302
        </td>
        <td>
          7576
          -
          7580
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          275
        </td>
        <td>
          7580
          -
          7595
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.hasherClassName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.hasherClassName
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          170
        </td>
        <td>
          7627
          -
          7637
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.length, &quot;
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          319
        </td>
        <td>
          7728
          -
          7765
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashInt(scala.StringContext.apply(&quot;&quot;, &quot; ? 1 : 0&quot;).s(input), result)
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          13
        </td>
        <td>
          7739
          -
          7756
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; ? 1 : 0&quot;).s(input)
        </td>
      </tr><tr>
        <td>
          196
        </td>
        <td>
          129
        </td>
        <td>
          7847
          -
          7860
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n       |if(&quot;
        </td>
      </tr><tr>
        <td>
          196
        </td>
        <td>
          9
        </td>
        <td>
          7843
          -
          8015
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n       |if(&quot;, &quot; == -0.0f) {\n       |  &quot;, &quot;\n       |} else {\n       |  &quot;, &quot;\n       |}\n     &quot;).s(input, HashLongsExpression.this.genHashInt(&quot;0&quot;, result), HashLongsExpression.this.genHashInt(scala.StringContext.apply(&quot;Float.floatToIntBits(&quot;, &quot;)&quot;).s(input), result))
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          38
        </td>
        <td>
          7865
          -
          7889
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; == -0.0f) {\n       |  &quot;
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          460
        </td>
        <td>
          7890
          -
          7913
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashInt(&quot;0&quot;, result)
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          345
        </td>
        <td>
          7914
          -
          7943
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n       |} else {\n       |  &quot;
        </td>
      </tr><tr>
        <td>
          200
        </td>
        <td>
          288
        </td>
        <td>
          7955
          -
          7986
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;Float.floatToIntBits(&quot;, &quot;)&quot;).s(input)
        </td>
      </tr><tr>
        <td>
          200
        </td>
        <td>
          186
        </td>
        <td>
          7944
          -
          7995
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashInt(scala.StringContext.apply(&quot;Float.floatToIntBits(&quot;, &quot;)&quot;).s(input), result)
        </td>
      </tr><tr>
        <td>
          200
        </td>
        <td>
          171
        </td>
        <td>
          7996
          -
          8015
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n       |}\n     &quot;
        </td>
      </tr><tr>
        <td>
          202
        </td>
        <td>
          320
        </td>
        <td>
          7843
          -
          8027
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.stripMargin
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.augmentString(scala.StringContext.apply(&quot;\n       |if(&quot;, &quot; == -0.0f) {\n       |  &quot;, &quot;\n       |} else {\n       |  &quot;, &quot;\n       |}\n     &quot;).s(input, HashLongsExpression.this.genHashInt(&quot;0&quot;, result), HashLongsExpression.this.genHashInt(scala.StringContext.apply(&quot;Float.floatToIntBits(&quot;, &quot;)&quot;).s(input), result))).stripMargin
        </td>
      </tr><tr>
        <td>
          206
        </td>
        <td>
          11
        </td>
        <td>
          8110
          -
          8288
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n       |if(&quot;, &quot; == -0.0d) {\n       |  &quot;, &quot;\n       |} else {\n       |  &quot;, &quot;\n       |}\n     &quot;).s(input, HashLongsExpression.this.genHashLong(&quot;0L&quot;, result), HashLongsExpression.this.genHashLong(scala.StringContext.apply(&quot;Double.doubleToLongBits(&quot;, &quot;)&quot;).s(input), result))
        </td>
      </tr><tr>
        <td>
          206
        </td>
        <td>
          145
        </td>
        <td>
          8114
          -
          8127
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n       |if(&quot;
        </td>
      </tr><tr>
        <td>
          207
        </td>
        <td>
          437
        </td>
        <td>
          8132
          -
          8156
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; == -0.0d) {\n       |  &quot;
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          346
        </td>
        <td>
          8183
          -
          8212
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n       |} else {\n       |  &quot;
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          462
        </td>
        <td>
          8157
          -
          8182
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashLong(&quot;0L&quot;, result)
        </td>
      </tr><tr>
        <td>
          210
        </td>
        <td>
          283
        </td>
        <td>
          8225
          -
          8259
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;Double.doubleToLongBits(&quot;, &quot;)&quot;).s(input)
        </td>
      </tr><tr>
        <td>
          210
        </td>
        <td>
          160
        </td>
        <td>
          8269
          -
          8288
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n       |}\n     &quot;
        </td>
      </tr><tr>
        <td>
          210
        </td>
        <td>
          187
        </td>
        <td>
          8213
          -
          8268
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashLong(scala.StringContext.apply(&quot;Double.doubleToLongBits(&quot;, &quot;)&quot;).s(input), result)
        </td>
      </tr><tr>
        <td>
          212
        </td>
        <td>
          312
        </td>
        <td>
          8110
          -
          8300
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.stripMargin
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.augmentString(scala.StringContext.apply(&quot;\n       |if(&quot;, &quot; == -0.0d) {\n       |  &quot;, &quot;\n       |} else {\n       |  &quot;, &quot;\n       |}\n     &quot;).s(input, HashLongsExpression.this.genHashLong(&quot;0L&quot;, result), HashLongsExpression.this.genHashLong(scala.StringContext.apply(&quot;Double.doubleToLongBits(&quot;, &quot;)&quot;).s(input), result))).stripMargin
        </td>
      </tr><tr>
        <td>
          220
        </td>
        <td>
          438
        </td>
        <td>
          8554
          -
          8592
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;=
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          d.precision.&lt;=(org.apache.spark.sql.types.Decimal.MAX_LONG_DIGITS)
        </td>
      </tr><tr>
        <td>
          220
        </td>
        <td>
          135
        </td>
        <td>
          8569
          -
          8592
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.Decimal.MAX_LONG_DIGITS
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.Decimal.MAX_LONG_DIGITS
        </td>
      </tr><tr>
        <td>
          221
        </td>
        <td>
          454
        </td>
        <td>
          8602
          -
          8649
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashLong(scala.StringContext.apply(&quot;&quot;, &quot;.toUnscaledLong()&quot;).s(input), result)
        </td>
      </tr><tr>
        <td>
          221
        </td>
        <td>
          324
        </td>
        <td>
          8614
          -
          8640
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;.toUnscaledLong()&quot;).s(input)
        </td>
      </tr><tr>
        <td>
          221
        </td>
        <td>
          161
        </td>
        <td>
          8602
          -
          8649
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashLong(scala.StringContext.apply(&quot;&quot;, &quot;.toUnscaledLong()&quot;).s(input), result)
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          461
        </td>
        <td>
          8661
          -
          8873
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  val bytes: String = ctx.freshName(&quot;bytes&quot;);
  scala.Predef.augmentString(scala.StringContext.apply(&quot;\n         |final byte[] &quot;, &quot; = &quot;, &quot;.toJavaBigDecimal().unscaledValue().toByteArray();\n         |&quot;, &quot;\n       &quot;).s(bytes, input, HashLongsExpression.this.genHashBytes(bytes, result))).stripMargin
}
        </td>
      </tr><tr>
        <td>
          223
        </td>
        <td>
          284
        </td>
        <td>
          8681
          -
          8703
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.freshName(&quot;bytes&quot;)
        </td>
      </tr><tr>
        <td>
          224
        </td>
        <td>
          325
        </td>
        <td>
          8710
          -
          8855
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n         |final byte[] &quot;, &quot; = &quot;, &quot;.toJavaBigDecimal().unscaledValue().toByteArray();\n         |&quot;, &quot;\n       &quot;).s(bytes, input, HashLongsExpression.this.genHashBytes(bytes, result))
        </td>
      </tr><tr>
        <td>
          224
        </td>
        <td>
          188
        </td>
        <td>
          8714
          -
          8739
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n         |final byte[] &quot;
        </td>
      </tr><tr>
        <td>
          225
        </td>
        <td>
          313
        </td>
        <td>
          8753
          -
          8815
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.toJavaBigDecimal().unscaledValue().toByteArray();\n         |&quot;
        </td>
      </tr><tr>
        <td>
          225
        </td>
        <td>
          3
        </td>
        <td>
          8744
          -
          8748
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          226
        </td>
        <td>
          432
        </td>
        <td>
          8816
          -
          8843
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashBytes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashBytes(bytes, result)
        </td>
      </tr><tr>
        <td>
          226
        </td>
        <td>
          143
        </td>
        <td>
          8844
          -
          8855
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n       &quot;
        </td>
      </tr><tr>
        <td>
          227
        </td>
        <td>
          151
        </td>
        <td>
          8710
          -
          8867
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.stripMargin
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.augmentString(scala.StringContext.apply(&quot;\n         |final byte[] &quot;, &quot; = &quot;, &quot;.toJavaBigDecimal().unscaledValue().toByteArray();\n         |&quot;, &quot;\n       &quot;).s(bytes, input, HashLongsExpression.this.genHashBytes(bytes, result))).stripMargin
        </td>
      </tr><tr>
        <td>
          231
        </td>
        <td>
          276
        </td>
        <td>
          8949
          -
          8971
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashLong(t, result)
        </td>
      </tr><tr>
        <td>
          234
        </td>
        <td>
          113
        </td>
        <td>
          9085
          -
          9086
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          234
        </td>
        <td>
          4
        </td>
        <td>
          9101
          -
          9112
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.hashLong(&quot;
        </td>
      </tr><tr>
        <td>
          234
        </td>
        <td>
          351
        </td>
        <td>
          9083
          -
          9141
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;.hashLong(&quot;, &quot;.microseconds, &quot;, &quot;)&quot;).s(HashLongsExpression.this.hasherClassName, input, result)
        </td>
      </tr><tr>
        <td>
          234
        </td>
        <td>
          422
        </td>
        <td>
          9086
          -
          9101
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.hasherClassName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.hasherClassName
        </td>
      </tr><tr>
        <td>
          234
        </td>
        <td>
          144
        </td>
        <td>
          9139
          -
          9141
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;)&quot;
        </td>
      </tr><tr>
        <td>
          234
        </td>
        <td>
          305
        </td>
        <td>
          9117
          -
          9133
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.microseconds, &quot;
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          146
        </td>
        <td>
          9146
          -
          9218
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; = &quot;, &quot;.hashInt(&quot;, &quot;.months, &quot;, &quot;);&quot;).s(result, HashLongsExpression.this.hasherClassName, input, microsecondsHash)
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          463
        </td>
        <td>
          9155
          -
          9159
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          295
        </td>
        <td>
          9174
          -
          9184
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.hashInt(&quot;
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          120
        </td>
        <td>
          9189
          -
          9199
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.months, &quot;
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          153
        </td>
        <td>
          9148
          -
          9149
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          27
        </td>
        <td>
          9215
          -
          9218
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;);&quot;
        </td>
      </tr><tr>
        <td>
          235
        </td>
        <td>
          317
        </td>
        <td>
          9159
          -
          9174
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.hasherClassName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.hasherClassName
        </td>
      </tr><tr>
        <td>
          239
        </td>
        <td>
          445
        </td>
        <td>
          9318
          -
          9343
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;.getBaseObject()&quot;).s(input)
        </td>
      </tr><tr>
        <td>
          240
        </td>
        <td>
          271
        </td>
        <td>
          9365
          -
          9390
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;.getBaseOffset()&quot;).s(input)
        </td>
      </tr><tr>
        <td>
          241
        </td>
        <td>
          172
        </td>
        <td>
          9410
          -
          9430
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot;.numBytes()&quot;).s(input)
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          433
        </td>
        <td>
          9524
          -
          9527
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;);&quot;
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          139
        </td>
        <td>
          9515
          -
          9518
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, &quot;
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          112
        </td>
        <td>
          9463
          -
          9481
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.hashUnsafeBytes(&quot;
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          318
        </td>
        <td>
          9504
          -
          9507
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, &quot;
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          297
        </td>
        <td>
          9444
          -
          9448
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          19
        </td>
        <td>
          9491
          -
          9494
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;, &quot;
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          261
        </td>
        <td>
          9448
          -
          9463
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.hasherClassName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.hasherClassName
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          458
        </td>
        <td>
          9437
          -
          9438
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          152
        </td>
        <td>
          9435
          -
          9527
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;&quot;, &quot; = &quot;, &quot;.hashUnsafeBytes(&quot;, &quot;, &quot;, &quot;, &quot;, &quot;, &quot;, &quot;);&quot;).s(result, HashLongsExpression.this.hasherClassName, baseObject, baseOffset, numBytes, result)
        </td>
      </tr><tr>
        <td>
          252
        </td>
        <td>
          459
        </td>
        <td>
          9898
          -
          9920
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.freshName(&quot;index&quot;)
        </td>
      </tr><tr>
        <td>
          253
        </td>
        <td>
          290
        </td>
        <td>
          9936
          -
          9957
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.freshName(&quot;keys&quot;)
        </td>
      </tr><tr>
        <td>
          254
        </td>
        <td>
          114
        </td>
        <td>
          9975
          -
          9998
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.freshName(&quot;values&quot;)
        </td>
      </tr><tr>
        <td>
          255
        </td>
        <td>
          265
        </td>
        <td>
          10003
          -
          10371
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n        final ArrayData &quot;, &quot; = &quot;, &quot;.keyArray();\n        final ArrayData &quot;, &quot; = &quot;, &quot;.valueArray();\n        for (int &quot;, &quot; = 0; &quot;, &quot; &lt; &quot;, &quot;.numElements(); &quot;, &quot;++) {\n          &quot;, &quot;\n          &quot;, &quot;\n        }\n      &quot;).s(keys, input, values, input, index, index, input, index, HashLongsExpression.this.nullSafeElementHash(keys, index, false, keyType, result, ctx), HashLongsExpression.this.nullSafeElementHash(values, index, valueContainsNull, valueType, result, ctx))
        </td>
      </tr><tr>
        <td>
          255
        </td>
        <td>
          14
        </td>
        <td>
          10007
          -
          10033
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n        final ArrayData &quot;
        </td>
      </tr><tr>
        <td>
          256
        </td>
        <td>
          304
        </td>
        <td>
          10037
          -
          10041
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          256
        </td>
        <td>
          133
        </td>
        <td>
          10046
          -
          10084
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.keyArray();\n        final ArrayData &quot;
        </td>
      </tr><tr>
        <td>
          257
        </td>
        <td>
          263
        </td>
        <td>
          10099
          -
          10132
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.valueArray();\n        for (int &quot;
        </td>
      </tr><tr>
        <td>
          257
        </td>
        <td>
          425
        </td>
        <td>
          10090
          -
          10094
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = &quot;
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          450
        </td>
        <td>
          10149
          -
          10153
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; &lt; &quot;
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          154
        </td>
        <td>
          10137
          -
          10144
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = 0; &quot;
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          103
        </td>
        <td>
          10180
          -
          10197
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;++) {\n          &quot;
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          296
        </td>
        <td>
          10158
          -
          10175
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.numElements(); &quot;
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          130
        </td>
        <td>
          10198
          -
          10259
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.nullSafeElementHash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.nullSafeElementHash(keys, index, false, keyType, result, ctx)
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          392
        </td>
        <td>
          10260
          -
          10272
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n          &quot;
        </td>
      </tr><tr>
        <td>
          260
        </td>
        <td>
          444
        </td>
        <td>
          10273
          -
          10350
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.nullSafeElementHash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.nullSafeElementHash(values, index, valueContainsNull, valueType, result, ctx)
        </td>
      </tr><tr>
        <td>
          260
        </td>
        <td>
          306
        </td>
        <td>
          10351
          -
          10371
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n        }\n      &quot;
        </td>
      </tr><tr>
        <td>
          271
        </td>
        <td>
          173
        </td>
        <td>
          10701
          -
          10723
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.freshName(&quot;index&quot;)
        </td>
      </tr><tr>
        <td>
          272
        </td>
        <td>
          451
        </td>
        <td>
          10732
          -
          10751
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n        for (int &quot;
        </td>
      </tr><tr>
        <td>
          272
        </td>
        <td>
          262
        </td>
        <td>
          10728
          -
          10911
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n        for (int &quot;, &quot; = 0; &quot;, &quot; &lt; &quot;, &quot;.numElements(); &quot;, &quot;++) {\n          &quot;, &quot;\n        }\n      &quot;).s(index, index, input, index, HashLongsExpression.this.nullSafeElementHash(input, index, containsNull, elementType, result, ctx))
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          289
        </td>
        <td>
          10756
          -
          10763
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; = 0; &quot;
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          124
        </td>
        <td>
          10768
          -
          10772
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot; &lt; &quot;
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          393
        </td>
        <td>
          10777
          -
          10794
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;.numElements(); &quot;
        </td>
      </tr><tr>
        <td>
          273
        </td>
        <td>
          321
        </td>
        <td>
          10799
          -
          10816
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;++) {\n          &quot;
        </td>
      </tr><tr>
        <td>
          274
        </td>
        <td>
          140
        </td>
        <td>
          10891
          -
          10911
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;\n        }\n      &quot;
        </td>
      </tr><tr>
        <td>
          274
        </td>
        <td>
          439
        </td>
        <td>
          10817
          -
          10890
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.nullSafeElementHash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.nullSafeElementHash(input, index, containsNull, elementType, result, ctx)
        </td>
      </tr><tr>
        <td>
          284
        </td>
        <td>
          148
        </td>
        <td>
          11198
          -
          11220
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.freshName
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.freshName(&quot;input&quot;)
        </td>
      </tr><tr>
        <td>
          285
        </td>
        <td>
          465
        </td>
        <td>
          11249
          -
          11249
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.this.Array.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]((ClassTag.apply[(org.apache.spark.sql.types.StructField, Int)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(org.apache.spark.sql.types.StructField, Int)]))
        </td>
      </tr><tr>
        <td>
          285
        </td>
        <td>
          264
        </td>
        <td>
          11242
          -
          11393
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.refArrayOps[(org.apache.spark.sql.types.StructField, Int)](scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](fields).zipWithIndex[org.apache.spark.sql.types.StructField, Array[(org.apache.spark.sql.types.StructField, Int)]](scala.this.Array.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]((ClassTag.apply[(org.apache.spark.sql.types.StructField, Int)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(org.apache.spark.sql.types.StructField, Int)])))).map[String, Array[String]](((x0$1: (org.apache.spark.sql.types.StructField, Int)) =&gt; x0$1 match {
  case (_1: org.apache.spark.sql.types.StructField, _2: Int)(org.apache.spark.sql.types.StructField, Int)((field @ _), (index @ _)) =&gt; HashLongsExpression.this.nullSafeElementHash(tmpInput, index.toString(), field.nullable, field.dataType, result, ctx)
}))(scala.this.Array.canBuildFrom[String]((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))
        </td>
      </tr><tr>
        <td>
          285
        </td>
        <td>
          291
        </td>
        <td>
          11242
          -
          11261
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.IndexedSeqOptimized.zipWithIndex
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](fields).zipWithIndex[org.apache.spark.sql.types.StructField, Array[(org.apache.spark.sql.types.StructField, Int)]](scala.this.Array.canBuildFrom[(org.apache.spark.sql.types.StructField, Int)]((ClassTag.apply[(org.apache.spark.sql.types.StructField, Int)](classOf[scala.Tuple2]): scala.reflect.ClassTag[(org.apache.spark.sql.types.StructField, Int)])))
        </td>
      </tr><tr>
        <td>
          285
        </td>
        <td>
          440
        </td>
        <td>
          11266
          -
          11266
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.this.Array.canBuildFrom[String]((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))
        </td>
      </tr><tr>
        <td>
          286
        </td>
        <td>
          125
        </td>
        <td>
          11327
          -
          11341
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Any.toString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          index.toString()
        </td>
      </tr><tr>
        <td>
          286
        </td>
        <td>
          136
        </td>
        <td>
          11297
          -
          11387
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.nullSafeElementHash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.nullSafeElementHash(tmpInput, index.toString(), field.nullable, field.dataType, result, ctx)
        </td>
      </tr><tr>
        <td>
          286
        </td>
        <td>
          411
        </td>
        <td>
          11343
          -
          11357
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.nullable
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          field.nullable
        </td>
      </tr><tr>
        <td>
          286
        </td>
        <td>
          299
        </td>
        <td>
          11359
          -
          11373
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          field.dataType
        </td>
      </tr><tr>
        <td>
          288
        </td>
        <td>
          456
        </td>
        <td>
          11419
          -
          11451
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.javaType(HashLongsExpression.this.dataType)
        </td>
      </tr><tr>
        <td>
          288
        </td>
        <td>
          84
        </td>
        <td>
          11442
          -
          11450
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.dataType
        </td>
      </tr><tr>
        <td>
          289
        </td>
        <td>
          464
        </td>
        <td>
          11467
          -
          11872
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.splitExpressions
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ctx.splitExpressions(scala.Predef.wrapRefArray[String](fieldsHash), &quot;computeHashForStruct&quot;, scala.collection.Seq.apply[(String, String)](scala.Predef.ArrowAssoc[String](&quot;InternalRow&quot;).-&gt;[String](tmpInput), scala.Predef.ArrowAssoc[String](hashResultType).-&gt;[String](result)), hashResultType, ((body: String) =&gt; scala.Predef.augmentString(scala.StringContext.apply(&quot;\n           |&quot;, &quot;\n           |return &quot;, &quot;;\n         &quot;).s(body, result)).stripMargin), ((x$3: Seq[String]) =&gt; x$3.map[String, Seq[String]](((funcCall: String) =&gt; scala.StringContext.apply(&quot;&quot;, &quot; = &quot;, &quot;;&quot;).s(result, funcCall)))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;\n&quot;)))
        </td>
      </tr><tr>
        <td>
          290
        </td>
        <td>
          285
        </td>
        <td>
          11509
          -
          11519
        </td>
        <td>
          ApplyImplicitView
        </td>
        <td>
          scala.LowPriorityImplicits.wrapRefArray
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.wrapRefArray[String](fieldsHash)
        </td>
      </tr><tr>
        <td>
          291
        </td>
        <td>
          121
        </td>
        <td>
          11538
          -
          11560
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;computeHashForStruct&quot;
        </td>
      </tr><tr>
        <td>
          292
        </td>
        <td>
          137
        </td>
        <td>
          11580
          -
          11636
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.generic.GenericCompanion.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.collection.Seq.apply[(String, String)](scala.Predef.ArrowAssoc[String](&quot;InternalRow&quot;).-&gt;[String](tmpInput), scala.Predef.ArrowAssoc[String](hashResultType).-&gt;[String](result))
        </td>
      </tr><tr>
        <td>
          292
        </td>
        <td>
          412
        </td>
        <td>
          11584
          -
          11609
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.ArrowAssoc[String](&quot;InternalRow&quot;).-&gt;[String](tmpInput)
        </td>
      </tr><tr>
        <td>
          292
        </td>
        <td>
          314
        </td>
        <td>
          11611
          -
          11635
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.ArrowAssoc.-&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.ArrowAssoc[String](hashResultType).-&gt;[String](result)
        </td>
      </tr><tr>
        <td>
          295
        </td>
        <td>
          442
        </td>
        <td>
          11715
          -
          11778
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n           |&quot;, &quot;\n           |return &quot;, &quot;;\n         &quot;).s(body, result)
        </td>
      </tr><tr>
        <td>
          298
        </td>
        <td>
          253
        </td>
        <td>
          11715
          -
          11790
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.stripMargin
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.augmentString(scala.StringContext.apply(&quot;\n           |&quot;, &quot;\n           |return &quot;, &quot;;\n         &quot;).s(body, result)).stripMargin
        </td>
      </tr><tr>
        <td>
          299
        </td>
        <td>
          85
        </td>
        <td>
          11814
          -
          11871
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.mkString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$3.map[String, Seq[String]](((funcCall: String) =&gt; scala.StringContext.apply(&quot;&quot;, &quot; = &quot;, &quot;;&quot;).s(result, funcCall)))(collection.this.Seq.canBuildFrom[String]).mkString(&quot;\n&quot;)
        </td>
      </tr><tr>
        <td>
          300
        </td>
        <td>
          287
        </td>
        <td>
          11877
          -
          11950
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.StringContext.s
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.StringContext.apply(&quot;\n       |final InternalRow &quot;, &quot; = &quot;, &quot;;\n       |&quot;, &quot;\n     &quot;).s(tmpInput, input, code)
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          104
        </td>
        <td>
          11877
          -
          11962
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.immutable.StringLike.stripMargin
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.augmentString(scala.StringContext.apply(&quot;\n       |final InternalRow &quot;, &quot; = &quot;, &quot;;\n       |&quot;, &quot;\n     &quot;).s(tmpInput, input, code)).stripMargin
        </td>
      </tr><tr>
        <td>
          312
        </td>
        <td>
          404
        </td>
        <td>
          12289
          -
          12291
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          &quot;&quot;
        </td>
      </tr><tr>
        <td>
          313
        </td>
        <td>
          214
        </td>
        <td>
          12316
          -
          12345
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashBoolean
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashBoolean(input, result)
        </td>
      </tr><tr>
        <td>
          314
        </td>
        <td>
          131
        </td>
        <td>
          12404
          -
          12429
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashInt(input, result)
        </td>
      </tr><tr>
        <td>
          315
        </td>
        <td>
          435
        </td>
        <td>
          12451
          -
          12477
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashLong(input, result)
        </td>
      </tr><tr>
        <td>
          316
        </td>
        <td>
          254
        </td>
        <td>
          12504
          -
          12535
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashTimestamp
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashTimestamp(input, result)
        </td>
      </tr><tr>
        <td>
          317
        </td>
        <td>
          78
        </td>
        <td>
          12558
          -
          12585
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashFloat
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashFloat(input, result)
        </td>
      </tr><tr>
        <td>
          318
        </td>
        <td>
          466
        </td>
        <td>
          12609
          -
          12637
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashDouble
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashDouble(input, result)
        </td>
      </tr><tr>
        <td>
          319
        </td>
        <td>
          277
        </td>
        <td>
          12665
          -
          12702
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashDecimal
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashDecimal(ctx, d, input, result)
        </td>
      </tr><tr>
        <td>
          320
        </td>
        <td>
          98
        </td>
        <td>
          12736
          -
          12774
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashCalendarInterval
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashCalendarInterval(input, result)
        </td>
      </tr><tr>
        <td>
          324
        </td>
        <td>
          407
        </td>
        <td>
          12955
          -
          12982
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashBytes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashBytes(input, result)
        </td>
      </tr><tr>
        <td>
          325
        </td>
        <td>
          240
        </td>
        <td>
          13006
          -
          13034
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashString
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashString(input, result)
        </td>
      </tr><tr>
        <td>
          326
        </td>
        <td>
          147
        </td>
        <td>
          13075
          -
          13128
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashForArray
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashForArray(ctx, input, result, et, containsNull)
        </td>
      </tr><tr>
        <td>
          328
        </td>
        <td>
          441
        </td>
        <td>
          13182
          -
          13242
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashForMap
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashForMap(ctx, input, result, kt, vt, valueContainsNull)
        </td>
      </tr><tr>
        <td>
          329
        </td>
        <td>
          246
        </td>
        <td>
          13274
          -
          13318
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.genHashForStruct
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.genHashForStruct(ctx, input, result, fields)
        </td>
      </tr><tr>
        <td>
          330
        </td>
        <td>
          94
        </td>
        <td>
          13385
          -
          13396
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.UserDefinedType.sqlType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          udt.sqlType
        </td>
      </tr><tr>
        <td>
          330
        </td>
        <td>
          446
        </td>
        <td>
          13355
          -
          13410
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.computeHashWithTailRec
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.computeHashWithTailRec(input, udt.sqlType, result, ctx)
        </td>
      </tr><tr>
        <td>
          337
        </td>
        <td>
          298
        </td>
        <td>
          13643
          -
          13695
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.HashLongsExpression.computeHashWithTailRec
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          HashLongsExpression.this.computeHashWithTailRec(input, dataType, result, ctx)
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          122
        </td>
        <td>
          14038
          -
          14071
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Int.toLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET.toLong
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          241
        </td>
        <td>
          13974
          -
          14071
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.getBaseObject().isInstanceOf[Array[Byte]].&amp;&amp;(s.getBaseOffset().&gt;=(org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET.toLong))
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          396
        </td>
        <td>
          14019
          -
          14071
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Long.&gt;=
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.getBaseOffset().&gt;=(org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET.toLong)
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          79
        </td>
        <td>
          14073
          -
          14406
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  val bytes: Array[Byte] = s.getBaseObject().asInstanceOf[Array[Byte]].asInstanceOf[Array[Byte]];
  val arrayOffset: Long = s.getBaseOffset().-(org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET.toLong);
  if (bytes.length.toLong.&lt;(arrayOffset.+(s.numBytes().toLong)))
    throw new scala.`package`.ArrayIndexOutOfBoundsException()
  else
    scala.Tuple3.apply[Array[Byte], Int, Int](bytes, arrayOffset.toInt, s.numBytes())
}
        </td>
      </tr><tr>
        <td>
          350
        </td>
        <td>
          127
        </td>
        <td>
          14093
          -
          14160
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Any.asInstanceOf
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.getBaseObject().asInstanceOf[Array[Byte]].asInstanceOf[Array[Byte]]
        </td>
      </tr><tr>
        <td>
          351
        </td>
        <td>
          272
        </td>
        <td>
          14185
          -
          14236
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Long.-
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.getBaseOffset().-(org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET.toLong)
        </td>
      </tr><tr>
        <td>
          351
        </td>
        <td>
          434
        </td>
        <td>
          14203
          -
          14236
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Int.toLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET.toLong
        </td>
      </tr><tr>
        <td>
          352
        </td>
        <td>
          95
        </td>
        <td>
          14283
          -
          14300
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Int.toLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.numBytes().toLong
        </td>
      </tr><tr>
        <td>
          352
        </td>
        <td>
          286
        </td>
        <td>
          14247
          -
          14300
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Long.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          bytes.length.toLong.&lt;(arrayOffset.+(s.numBytes().toLong))
        </td>
      </tr><tr>
        <td>
          352
        </td>
        <td>
          385
        </td>
        <td>
          14269
          -
          14300
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Long.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          arrayOffset.+(s.numBytes().toLong)
        </td>
      </tr><tr>
        <td>
          352
        </td>
        <td>
          115
        </td>
        <td>
          14302
          -
          14342
        </td>
        <td>
          Throw
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          throw new scala.`package`.ArrayIndexOutOfBoundsException()
        </td>
      </tr><tr>
        <td>
          352
        </td>
        <td>
          405
        </td>
        <td>
          14302
          -
          14342
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          throw new scala.`package`.ArrayIndexOutOfBoundsException()
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          436
        </td>
        <td>
          14362
          -
          14400
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Tuple3.apply[Array[Byte], Int, Int](bytes, arrayOffset.toInt, s.numBytes())
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          142
        </td>
        <td>
          14389
          -
          14399
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.numBytes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.numBytes()
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          266
        </td>
        <td>
          14362
          -
          14400
        </td>
        <td>
          Block
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Tuple3.apply[Array[Byte], Int, Int](bytes, arrayOffset.toInt, s.numBytes())
        </td>
      </tr><tr>
        <td>
          354
        </td>
        <td>
          242
        </td>
        <td>
          14370
          -
          14387
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Long.toInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          arrayOffset.toInt
        </td>
      </tr><tr>
        <td>
          357
        </td>
        <td>
          406
        </td>
        <td>
          14424
          -
          14451
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Tuple3.apply[Array[Byte], Int, Int](s.getBytes(), 0, s.numBytes())
        </td>
      </tr><tr>
        <td>
          357
        </td>
        <td>
          352
        </td>
        <td>
          14425
          -
          14435
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.getBytes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.getBytes()
        </td>
      </tr><tr>
        <td>
          357
        </td>
        <td>
          280
        </td>
        <td>
          14437
          -
          14438
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          357
        </td>
        <td>
          118
        </td>
        <td>
          14440
          -
          14450
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.unsafe.types.UTF8String.numBytes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.numBytes()
        </td>
      </tr><tr>
        <td>
          357
        </td>
        <td>
          221
        </td>
        <td>
          14424
          -
          14451
        </td>
        <td>
          Block
        </td>
        <td>
          scala.Tuple3.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Tuple3.apply[Array[Byte], Int, Int](s.getBytes(), 0, s.numBytes())
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          427
        </td>
        <td>
          15034
          -
          15035
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          1
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          134
        </td>
        <td>
          15034
          -
          15035
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          1
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          80
        </td>
        <td>
          15041
          -
          15042
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          268
        </td>
        <td>
          15041
          -
          15042
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          379
        </td>
        <td>
          362
        </td>
        <td>
          15019
          -
          15051
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashInt(if (b)
  1
else
  0, digest)
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          107
        </td>
        <td>
          15074
          -
          15092
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashInt(b.toInt, digest)
        </td>
      </tr><tr>
        <td>
          380
        </td>
        <td>
          282
        </td>
        <td>
          15082
          -
          15083
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Byte.toInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          b.toInt
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          394
        </td>
        <td>
          15124
          -
          15125
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Short.toInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          s.toInt
        </td>
      </tr><tr>
        <td>
          381
        </td>
        <td>
          223
        </td>
        <td>
          15116
          -
          15134
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashInt(s.toInt, digest)
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          60
        </td>
        <td>
          15156
          -
          15174
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashInt(i, digest)
        </td>
      </tr><tr>
        <td>
          383
        </td>
        <td>
          429
        </td>
        <td>
          15197
          -
          15216
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashLong(l, digest)
        </td>
      </tr><tr>
        <td>
          384
        </td>
        <td>
          71
        </td>
        <td>
          15256
          -
          15274
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashInt(0, digest)
        </td>
      </tr><tr>
        <td>
          384
        </td>
        <td>
          248
        </td>
        <td>
          15241
          -
          15251
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Float.==
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          f.==(-0.0)
        </td>
      </tr><tr>
        <td>
          385
        </td>
        <td>
          292
        </td>
        <td>
          15298
          -
          15348
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hashInt
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashInt(java.lang.Float.floatToIntBits(f), digest)
        </td>
      </tr><tr>
        <td>
          385
        </td>
        <td>
          383
        </td>
        <td>
          15306
          -
          15339
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Float.floatToIntBits
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          java.lang.Float.floatToIntBits(f)
        </td>
      </tr><tr>
        <td>
          386
        </td>
        <td>
          110
        </td>
        <td>
          15374
          -
          15384
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Double.==
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          d.==(-0.0)
        </td>
      </tr><tr>
        <td>
          386
        </td>
        <td>
          397
        </td>
        <td>
          15389
          -
          15409
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashLong(0L, digest)
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          217
        </td>
        <td>
          15443
          -
          15479
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Double.doubleToLongBits
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          java.lang.Double.doubleToLongBits(d)
        </td>
      </tr><tr>
        <td>
          387
        </td>
        <td>
          61
        </td>
        <td>
          15434
          -
          15488
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashLong(java.lang.Double.doubleToLongBits(d), digest)
        </td>
      </tr><tr>
        <td>
          389
        </td>
        <td>
          421
        </td>
        <td>
          15538
          -
          15582
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.DecimalType.precision
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          dataType.asInstanceOf[org.apache.spark.sql.types.DecimalType].precision
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          74
        </td>
        <td>
          15595
          -
          15631
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;=
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          precision.&lt;=(org.apache.spark.sql.types.Decimal.MAX_LONG_DIGITS)
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          244
        </td>
        <td>
          15608
          -
          15631
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.Decimal.MAX_LONG_DIGITS
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.types.Decimal.MAX_LONG_DIGITS
        </td>
      </tr><tr>
        <td>
          391
        </td>
        <td>
          373
        </td>
        <td>
          15654
          -
          15670
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.Decimal.toUnscaledLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          d.toUnscaledLong
        </td>
      </tr><tr>
        <td>
          391
        </td>
        <td>
          116
        </td>
        <td>
          15645
          -
          15679
        </td>
        <td>
          Block
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashLong(d.toUnscaledLong, digest)
        </td>
      </tr><tr>
        <td>
          391
        </td>
        <td>
          205
        </td>
        <td>
          15645
          -
          15679
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hashLong
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashLong(d.toUnscaledLong, digest)
        </td>
      </tr><tr>
        <td>
          392
        </td>
        <td>
          269
        </td>
        <td>
          15695
          -
          15852
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  val bytes: Array[Byte] = d.toJavaBigDecimal.unscaledValue().toByteArray();
  InterpretedHashLongsFunction.this.hashBytes(bytes, org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET, bytes.length, digest)
}
        </td>
      </tr><tr>
        <td>
          393
        </td>
        <td>
          389
        </td>
        <td>
          15719
          -
          15765
        </td>
        <td>
          Apply
        </td>
        <td>
          java.math.BigInteger.toByteArray
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          d.toJavaBigDecimal.unscaledValue().toByteArray()
        </td>
      </tr><tr>
        <td>
          394
        </td>
        <td>
          232
        </td>
        <td>
          15793
          -
          15819
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET
        </td>
      </tr><tr>
        <td>
          394
        </td>
        <td>
          443
        </td>
        <td>
          15776
          -
          15842
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hashBytes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashBytes(bytes, org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET, bytes.length, digest)
        </td>
      </tr><tr>
        <td>
          394
        </td>
        <td>
          48
        </td>
        <td>
          15821
          -
          15833
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Array.length
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          bytes.length
        </td>
      </tr><tr>
        <td>
          397
        </td>
        <td>
          64
        </td>
        <td>
          15895
          -
          15942
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.ShimUtils.hashCalendarInterval
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.sql.ShimUtils.hashCalendarInterval(c, this, digest)
        </td>
      </tr><tr>
        <td>
          399
        </td>
        <td>
          375
        </td>
        <td>
          15993
          -
          16019
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET
        </td>
      </tr><tr>
        <td>
          399
        </td>
        <td>
          174
        </td>
        <td>
          16021
          -
          16029
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Array.length
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          a.length
        </td>
      </tr><tr>
        <td>
          399
        </td>
        <td>
          105
        </td>
        <td>
          15980
          -
          16038
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hashBytes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashBytes(a, org.apache.spark.unsafe.Platform.BYTE_ARRAY_OFFSET, a.length, digest)
        </td>
      </tr><tr>
        <td>
          402
        </td>
        <td>
          414
        </td>
        <td>
          16096
          -
          16096
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._1
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$4._1
        </td>
      </tr><tr>
        <td>
          402
        </td>
        <td>
          225
        </td>
        <td>
          16103
          -
          16103
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._2
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$4._2
        </td>
      </tr><tr>
        <td>
          402
        </td>
        <td>
          51
        </td>
        <td>
          16111
          -
          16111
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple3._3
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$4._3
        </td>
      </tr><tr>
        <td>
          404
        </td>
        <td>
          430
        </td>
        <td>
          16161
          -
          16203
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hashBytes
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hashBytes(bytes, offset, numbytes, digest)
        </td>
      </tr><tr>
        <td>
          410
        </td>
        <td>
          255
        </td>
        <td>
          16330
          -
          16377
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.ArrayType.elementType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          udt.sqlType.asInstanceOf[org.apache.spark.sql.types.ArrayType].elementType
        </td>
      </tr><tr>
        <td>
          414
        </td>
        <td>
          69
        </td>
        <td>
          16470
          -
          16471
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          415
        </td>
        <td>
          331
        </td>
        <td>
          16512
          -
          16612
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  {
    result = InterpretedHashLongsFunction.this.hash(array.get(i, elementType), elementType, result);
    i = i.+(1)
  };
  while$2()
}
        </td>
      </tr><tr>
        <td>
          415
        </td>
        <td>
          55
        </td>
        <td>
          16512
          -
          16512
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.while$2
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          while$2()
        </td>
      </tr><tr>
        <td>
          415
        </td>
        <td>
          258
        </td>
        <td>
          16480
          -
          16480
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          415
        </td>
        <td>
          201
        </td>
        <td>
          16487
          -
          16510
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          i.&lt;(array.numElements())
        </td>
      </tr><tr>
        <td>
          415
        </td>
        <td>
          96
        </td>
        <td>
          16480
          -
          16480
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          415
        </td>
        <td>
          365
        </td>
        <td>
          16491
          -
          16510
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.ArrayData.numElements
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          array.numElements()
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          109
        </td>
        <td>
          16538
          -
          16563
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.SpecializedGetters.get
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          array.get(i, elementType)
        </td>
      </tr><tr>
        <td>
          416
        </td>
        <td>
          408
        </td>
        <td>
          16533
          -
          16585
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hash(array.get(i, elementType), elementType, result)
        </td>
      </tr><tr>
        <td>
          417
        </td>
        <td>
          215
        </td>
        <td>
          16596
          -
          16602
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          i.+(1)
        </td>
      </tr><tr>
        <td>
          422
        </td>
        <td>
          358
        </td>
        <td>
          16669
          -
          16669
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$5._1
        </td>
      </tr><tr>
        <td>
          422
        </td>
        <td>
          179
        </td>
        <td>
          16673
          -
          16673
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$5._2
        </td>
      </tr><tr>
        <td>
          428
        </td>
        <td>
          99
        </td>
        <td>
          16922
          -
          16936
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.MapData.keyArray
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          map.keyArray()
        </td>
      </tr><tr>
        <td>
          429
        </td>
        <td>
          387
        </td>
        <td>
          16958
          -
          16974
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.MapData.valueArray
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          map.valueArray()
        </td>
      </tr><tr>
        <td>
          431
        </td>
        <td>
          218
        </td>
        <td>
          17019
          -
          17020
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          432
        </td>
        <td>
          49
        </td>
        <td>
          17029
          -
          17029
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          432
        </td>
        <td>
          417
        </td>
        <td>
          17059
          -
          17059
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.while$3
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          while$3()
        </td>
      </tr><tr>
        <td>
          432
        </td>
        <td>
          207
        </td>
        <td>
          17059
          -
          17195
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  {
    result = InterpretedHashLongsFunction.this.hash(keys.get(i, kt), kt, result);
    result = InterpretedHashLongsFunction.this.hash(values.get(i, vt), vt, result);
    i = i.+(1)
  };
  while$3()
}
        </td>
      </tr><tr>
        <td>
          432
        </td>
        <td>
          332
        </td>
        <td>
          17036
          -
          17057
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          i.&lt;(map.numElements())
        </td>
      </tr><tr>
        <td>
          432
        </td>
        <td>
          33
        </td>
        <td>
          17040
          -
          17057
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.util.MapData.numElements
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          map.numElements()
        </td>
      </tr><tr>
        <td>
          432
        </td>
        <td>
          347
        </td>
        <td>
          17029
          -
          17029
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          433
        </td>
        <td>
          62
        </td>
        <td>
          17080
          -
          17113
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hash(keys.get(i, kt), kt, result)
        </td>
      </tr><tr>
        <td>
          433
        </td>
        <td>
          249
        </td>
        <td>
          17085
          -
          17100
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.SpecializedGetters.get
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          keys.get(i, kt)
        </td>
      </tr><tr>
        <td>
          434
        </td>
        <td>
          360
        </td>
        <td>
          17138
          -
          17155
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.SpecializedGetters.get
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          values.get(i, vt)
        </td>
      </tr><tr>
        <td>
          434
        </td>
        <td>
          197
        </td>
        <td>
          17133
          -
          17168
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hash(values.get(i, vt), vt, result)
        </td>
      </tr><tr>
        <td>
          435
        </td>
        <td>
          101
        </td>
        <td>
          17179
          -
          17185
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          i.+(1)
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          65
        </td>
        <td>
          17394
          -
          17394
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.DataType]
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          354
        </td>
        <td>
          17354
          -
          17414
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.toArray
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          udt.sqlType.asInstanceOf[org.apache.spark.sql.types.StructType].map[org.apache.spark.sql.types.DataType, Seq[org.apache.spark.sql.types.DataType]](((x$6: org.apache.spark.sql.types.StructField) =&gt; x$6.dataType))(collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.DataType]).toArray[org.apache.spark.sql.types.DataType]((ClassTag.apply[org.apache.spark.sql.types.DataType](classOf[org.apache.spark.sql.types.DataType]): scala.reflect.ClassTag[org.apache.spark.sql.types.DataType]))
        </td>
      </tr><tr>
        <td>
          442
        </td>
        <td>
          251
        </td>
        <td>
          17395
          -
          17405
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$6.dataType
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          418
        </td>
        <td>
          17452
          -
          17474
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.Predef.refArrayOps[org.apache.spark.sql.types.StructField](fields).map[org.apache.spark.sql.types.DataType, Array[org.apache.spark.sql.types.DataType]](((x$7: org.apache.spark.sql.types.StructField) =&gt; x$7.dataType))(scala.this.Array.canBuildFrom[org.apache.spark.sql.types.DataType]((ClassTag.apply[org.apache.spark.sql.types.DataType](classOf[org.apache.spark.sql.types.DataType]): scala.reflect.ClassTag[org.apache.spark.sql.types.DataType])))
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          199
        </td>
        <td>
          17463
          -
          17473
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.types.StructField.dataType
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          x$7.dataType
        </td>
      </tr><tr>
        <td>
          443
        </td>
        <td>
          25
        </td>
        <td>
          17462
          -
          17462
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          scala.this.Array.canBuildFrom[org.apache.spark.sql.types.DataType]((ClassTag.apply[org.apache.spark.sql.types.DataType](classOf[org.apache.spark.sql.types.DataType]): scala.reflect.ClassTag[org.apache.spark.sql.types.DataType]))
        </td>
      </tr><tr>
        <td>
          446
        </td>
        <td>
          210
        </td>
        <td>
          17529
          -
          17530
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          0
        </td>
      </tr><tr>
        <td>
          447
        </td>
        <td>
          40
        </td>
        <td>
          17549
          -
          17565
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.catalyst.InternalRow.numFields
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          struct.numFields
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          236
        </td>
        <td>
          17590
          -
          17685
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          {
  {
    result = InterpretedHashLongsFunction.this.hash(struct.get(i, types.apply(i)), types.apply(i), result);
    i = i.+(1)
  };
  while$4()
}
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          409
        </td>
        <td>
          17590
          -
          17590
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.while$4
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          while$4()
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          41
        </td>
        <td>
          17574
          -
          17574
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          348
        </td>
        <td>
          17581
          -
          17588
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          i.&lt;(len)
        </td>
      </tr><tr>
        <td>
          448
        </td>
        <td>
          339
        </td>
        <td>
          17574
          -
          17574
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          449
        </td>
        <td>
          256
        </td>
        <td>
          17630
          -
          17638
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Array.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          types.apply(i)
        </td>
      </tr><tr>
        <td>
          449
        </td>
        <td>
          366
        </td>
        <td>
          17641
          -
          17649
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Array.apply
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          types.apply(i)
        </td>
      </tr><tr>
        <td>
          449
        </td>
        <td>
          189
        </td>
        <td>
          17611
          -
          17658
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.shim.hash.InterpretedHashLongsFunction.hash
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          InterpretedHashLongsFunction.this.hash(struct.get(i, types.apply(i)), types.apply(i), result)
        </td>
      </tr><tr>
        <td>
          449
        </td>
        <td>
          90
        </td>
        <td>
          17616
          -
          17639
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.SpecializedGetters.get
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          struct.get(i, types.apply(i))
        </td>
      </tr><tr>
        <td>
          450
        </td>
        <td>
          26
        </td>
        <td>
          17669
          -
          17675
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td>
          
        </td>
        <td style="background: #F0ADAD">
          i.+(1)
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>